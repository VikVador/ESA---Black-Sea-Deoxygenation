{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a999b204-3b95-404f-93a1-90b1bda33abb",
   "metadata": {},
   "source": [
    "<img src=\"../assets/header_notebook.png\" />\n",
    "<hr style=\"color:#5A7D9F;\">\n",
    "<p align=\"center\">\n",
    "    <b style=\"font-size:2vw; color:#5A7D9F; font-weight:bold;\">\n",
    "    <center>ESA - Black Sea Deoxygenation Emulator</center>\n",
    "    </b>\n",
    "</p>\n",
    "<hr style=\"color:#5A7D9F;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b55dcb8-9067-463c-b876-2e45565d6240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------\n",
    "# Librairies\n",
    "# ----------\n",
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import dawgz\n",
    "import wandb\n",
    "import xarray\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Dawgz (jobs //)\n",
    "from dawgz import job, schedule\n",
    "\n",
    "# -------------------\n",
    "# Librairies (Custom)\n",
    "# -------------------\n",
    "# Adding path to source folder to load custom modules\n",
    "sys.path.append('/src')\n",
    "sys.path.append('/src/debs/')\n",
    "sys.path.insert(1, '/src/debs/')\n",
    "sys.path.insert(1, '/scripts/')\n",
    "\n",
    "# Moving to the .py directory\n",
    "%cd src/debs/\n",
    "\n",
    "## Loading libraries\n",
    "from metrics     import *\n",
    "from dataset     import *\n",
    "from dataloader  import *\n",
    "from tools       import *\n",
    "from losses      import *\n",
    "\n",
    "# -------\n",
    "# Jupyter\n",
    "# -------\n",
    "%matplotlib inline\n",
    "plt.rcParams.update({'font.size': 13})\n",
    "\n",
    "# Making sure modules are reloaded when modified\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9c007b-1cab-4407-a5f0-cbcdd836a569",
   "metadata": {},
   "source": [
    "<hr style=\"color:#5A7D9F;\">\n",
    "<p align=\"center\">\n",
    "    <b style=\"font-size:2vw; color:#5A7D9F; font-weight:bold;\">\n",
    "    <center>Scripts</center>\n",
    "    </b>\n",
    "</p>\n",
    "<hr style=\"color:#5A7D9F;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfe6b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating the preprocessed data\n",
    "%run __generate.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8f46e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating the distributions of the data each month\n",
    "%run __distributions.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23f012a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training a neural network:\n",
    "%run __training.py --config local"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ebd86a",
   "metadata": {},
   "source": [
    "<hr style=\"color:#5A7D9F;\">\n",
    "<p align=\"center\">\n",
    "    <b style=\"font-size:2vw; color:#5A7D9F; font-weight:bold;\">\n",
    "    <center>Data</center>\n",
    "    </b>\n",
    "</p>\n",
    "<hr style=\"color:#5A7D9F;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb72cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the different inputs\n",
    "BSD_dataset = BlackSea_Dataset(year_start  = 2020,\n",
    "                               year_end    = 2021,\n",
    "                               month_start = 1,\n",
    "                               month_end   = 12)\n",
    "\n",
    "data_temperature   = BSD_dataset.get_data(variable = \"temperature\")\n",
    "data_salinity      = BSD_dataset.get_data(variable = \"salinity\")\n",
    "data_chlorophyll   = BSD_dataset.get_data(variable = \"chlorophyll\")\n",
    "data_kshort        = BSD_dataset.get_data(variable = \"kshort\")\n",
    "data_klong         = BSD_dataset.get_data(variable = \"klong\")\n",
    "data_oxygen        = BSD_dataset.get_data(variable = \"oxygen\")\n",
    "mask               = BSD_dataset.get_mask(False)\n",
    "maskCS             = BSD_dataset.get_mask(True)\n",
    "\n",
    "def generate_animation(data : np.array, mask : np.array, title : str, limits : list = [0, 1]):\n",
    "    \"\"\"Generate an animation of the data, i.e. used to inspect the data\"\"\"\n",
    "    import os\n",
    "    import cv2\n",
    "    import imageio\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    from PIL import Image\n",
    "    from matplotlib.patches import Rectangle\n",
    "    from datetime import datetime, timedelta\n",
    "\n",
    "    # Converting to booleans for easier plot\n",
    "    mask = mask > 0.5\n",
    "\n",
    "    # Displaying information over terminal\n",
    "    print(f\"Data shape: {data.shape}\")\n",
    "\n",
    "    def index_to_date(i):\n",
    "        \"\"\"Used to convert an index to a date, i.e. useful for the title of the plot\"\"\"\n",
    "\n",
    "        # Define the start date\n",
    "        start_date = datetime(2020, 1, 1)\n",
    "\n",
    "        # Calculate the offset from the start date\n",
    "        delta = timedelta(days=i)\n",
    "\n",
    "        # Calculate the corresponding date\n",
    "        result_date = start_date + delta\n",
    "\n",
    "        # Format the date as a string in the \"YYYY-MM-DD\" format\n",
    "        return result_date.strftime('%Y-%m-%d')\n",
    "\n",
    "    # Creation of a folder if it does not exist\n",
    "    if not os.path.exists(f'../../analysis/images/{title}'):\n",
    "        os.mkdir(f'../../analysis/images/{title}/')\n",
    "\n",
    "    # Displaying information over terminal\n",
    "    print(f\"Generating images for {title}...\")\n",
    "\n",
    "    # Creating of the plots\n",
    "    for i in range(data.shape[0]):\n",
    "        plt.figure(figsize = (12, 8))\n",
    "        plt.imshow(data[i], cmap='viridis', vmin = limits[0], vmax = limits[1])\n",
    "        plt.colorbar(fraction = 0.021)\n",
    "        plt.imshow(mask, cmap='gray', alpha=0.025)\n",
    "        date_string = index_to_date(i)\n",
    "        ax = plt.gca()\n",
    "        box = Rectangle((0.85, 0.9), 0.46, 0.16, edgecolor='black', facecolor='black', transform=ax.transAxes)\n",
    "        plt.annotate(date_string, xy=(0.98, 0.96), xycoords='axes fraction',\n",
    "                    horizontalalignment='right', verticalalignment='top',\n",
    "                    fontsize=12, color='white')\n",
    "        ax.add_patch(box)\n",
    "        plt.grid(True, alpha=0.1)\n",
    "        plt.savefig(f'../../analysis/images/{title}/{title}_{i}.png')\n",
    "        plt.close()\n",
    "\n",
    "    # Displaying information over terminal\n",
    "    print(\"Loading images...\")\n",
    "\n",
    "    # Creation of the paths and opening the plots\n",
    "    paths = [f\"../../analysis/images/{title}/{title}_{i}.png\" for i in range(data.shape[0])]\n",
    "    image_array = []\n",
    "    for my_file in paths:\n",
    "        image = Image.open(my_file)\n",
    "        image_array.append(image)\n",
    "\n",
    "    # Displaying information over terminal\n",
    "    print(\"Generating the video...\")\n",
    "\n",
    "    # Generating the video\n",
    "    with imageio.get_writer(f'../../analysis/images/{title}.gif', mode='I') as writer:\n",
    "        for filename in paths:\n",
    "            image = imageio.imread(filename)\n",
    "            writer.append_data(image)\n",
    "\n",
    "# Generating the animations\n",
    "generate_animation(data_oxygen,      maskCS,      \"oxygen\", limits = [0, 1])\n",
    "generate_animation(data_temperature,   mask, \"temperature\", limits = [0, 1])\n",
    "generate_animation(data_salinity,      mask,    \"salinity\", limits = [0, 1])\n",
    "generate_animation(data_chlorophyll,   mask, \"chlorophyll\", limits = [0, 0.25])\n",
    "generate_animation(data_kshort,        mask,      \"kshort\", limits = [0, 0.1])\n",
    "generate_animation(data_klong,         mask,       \"klong\", limits = [0, 0.1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7078a3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the different inputs\n",
    "BSD_dataset = BlackSea_Dataset(year_start  = 2010,\n",
    "                               year_end    = 2020,\n",
    "                               month_start = 1,\n",
    "                               month_end   = 12)\n",
    "\n",
    "data_oxygen        = BSD_dataset.get_data(variable = \"oxygen\")\n",
    "mask               = BSD_dataset.get_mask(False)\n",
    "maskCS             = BSD_dataset.get_mask(True)\n",
    "\n",
    "# Extracting the training set\n",
    "training_set = data_oxygen[: 365 * 6]\n",
    "\n",
    "# Computing the mean\n",
    "mean_6years = np.mean(training_set, axis = 0)\n",
    "\n",
    "# Stores the mean each year\n",
    "mean_each_year = [np.mean(data_oxygen[i * 365 : (i + 1) * 365], axis = 0) for i in range(6)]\n",
    "\n",
    "def save_plot(data : np.array, mask : np.array, title : str):\n",
    "    \"\"\"Save the plot of the mean\"\"\"\n",
    "    plt.figure(figsize = (12, 8))\n",
    "    plt.imshow(data, cmap='viridis', vmin = 0, vmax = 1)\n",
    "    plt.colorbar(fraction = 0.021)\n",
    "    plt.imshow(mask, cmap='gray', alpha = 0.025)\n",
    "    date_string = f\"Mean : {title}\"\n",
    "    ax = plt.gca()\n",
    "    box = Rectangle((0.80, 0.90), 0.56, 0.16, edgecolor='black', facecolor='black', transform=ax.transAxes)\n",
    "    plt.annotate(date_string, xy=(0.98, 0.965), xycoords='axes fraction',\n",
    "                horizontalalignment='right', verticalalignment='top',\n",
    "                fontsize=12, color='white')\n",
    "    ax.add_patch(box)\n",
    "    plt.grid(True, alpha=0.1)\n",
    "    plt.savefig(f'../../analysis/means/means_{title}.png')\n",
    "\n",
    "# Saving the plot for all the years\n",
    "save_plot(mean_6years, maskCS, \"Training\")\n",
    "\n",
    "# Saving the plot for each year\n",
    "for i, n in enumerate([\"2010\", \"2011\", \"2012\", \"2013\", \"2014\", \"2015\"]):\n",
    "    save_plot(mean_each_year[i], maskCS, n)\n",
    "\n",
    "# Normalized deoxygenation treshold\n",
    "hypox_tresh = xarray.open_dataset(BSD_dataset.paths[0])[\"HYPON\"].data.item()\n",
    "\n",
    "# Computing the average state of the region (Hypoxia or not)\n",
    "mean_6years_state = mean_6years < hypox_tresh\n",
    "\n",
    "# Stores the mean each year\n",
    "mean_each_year_state = [mean < hypox_tresh for mean in mean_each_year]\n",
    "\n",
    "def save_plot_state(data : np.array, mask : np.array, title : str):\n",
    "    \"\"\"Save the plot of the mean\"\"\"\n",
    "    plt.figure(figsize = (12, 8))\n",
    "    plt.imshow(data, cmap='viridis', vmin = 0, vmax = 1)\n",
    "    cbar = plt.colorbar(fraction=0.021)\n",
    "    cbar.set_ticks([0, 1])\n",
    "    cbar.set_ticklabels(['Oxygenated', 'Hypoxia'])\n",
    "    plt.imshow(mask, cmap='gray', alpha = 0.25)\n",
    "    date_string = f\"Mean : {title}\"\n",
    "    ax = plt.gca()\n",
    "    box = Rectangle((0.80, 0.90), 0.56, 0.16, edgecolor='black', facecolor='black', transform=ax.transAxes)\n",
    "    plt.annotate(date_string, xy=(0.98, 0.965), xycoords='axes fraction',\n",
    "                horizontalalignment='right', verticalalignment='top',\n",
    "                fontsize=12, color='white')\n",
    "    ax.add_patch(box)\n",
    "    plt.grid(True, alpha=0.1)\n",
    "    plt.savefig(f'../../analysis/means/means_{title}.png')\n",
    "\n",
    "# Saving the plot for all the years\n",
    "save_plot_state(mean_6years_state, maskCS, \"Training (H)\")\n",
    "\n",
    "# Saving the plot for each year\n",
    "for i, n in enumerate([\"2010 (H)\", \"2011 (H)\", \"2012 (H)\", \"2013 (H)\", \"2014 (H)\", \"2015 (H)\"]):\n",
    "    save_plot_state(mean_each_year_state[i], maskCS, n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0de6e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sending everything to wandb\n",
    "wandb.init(project = \"ESA - Repport\")\n",
    "\n",
    "# Sending the animations\n",
    "wandb.log({\"Oxygen\":              wandb.Video(\"../../analysis/images/oxygen.gif\", fps = 1),\n",
    "           \"Temperature\":         wandb.Video(\"../../analysis/images/temperature.gif\", fps = 1),\n",
    "           \"Salinity\":            wandb.Video(\"../../analysis/images/salinity.gif\", fps = 1),\n",
    "           \"Chlorophyll\":         wandb.Video(\"../../analysis/images/chlorophyll.gif\", fps = 1),\n",
    "           \"Reflectance (Short)\": wandb.Video(\"../../analysis/images/kshort.gif\", fps = 1),\n",
    "           \"Reflectance (Long)\" : wandb.Video(\"../../analysis/images/klong.gif\", fps = 1)})\n",
    "\n",
    "# Sending the mean concentrations\n",
    "wandb.log({\"Concentration (Mean, 2010-2015)\": wandb.Image(f\"../../analysis/means/means_Training.png\"),\n",
    "           \"Concentration (Mean, 2010)\":      wandb.Image(f\"../../analysis/means/means_2010.png\"),\n",
    "           \"Concentration (Mean, 2011)\":      wandb.Image(f\"../../analysis/means/means_2011.png\"),\n",
    "           \"Concentration (Mean, 2012)\":      wandb.Image(f\"../../analysis/means/means_2012.png\"),\n",
    "           \"Concentration (Mean, 2013)\":      wandb.Image(f\"../../analysis/means/means_2013.png\"),\n",
    "           \"Concentration (Mean, 2014)\":      wandb.Image(f\"../../analysis/means/means_2014.png\"),\n",
    "           \"Concentration (Mean, 2015)\":      wandb.Image(f\"../../analysis/means/means_2015.png\")})\n",
    "\n",
    "# Sending the mean states\n",
    "wandb.log({\"State (Mean, 2010-2015)\": wandb.Image(f\"../../analysis/means/means_Training_(H).png\"),\n",
    "           \"State (Mean, 2010)\":      wandb.Image(f\"../../analysis/means/means_2010_(H).png\"),\n",
    "           \"State (Mean, 2011)\":      wandb.Image(f\"../../analysis/means/means_2011_(H).png\"),\n",
    "           \"State (Mean, 2012)\":      wandb.Image(f\"../../analysis/means/means_2012_(H).png\"),\n",
    "           \"State (Mean, 2013)\":      wandb.Image(f\"../../analysis/means/means_2013_(H).png\"),\n",
    "           \"State (Mean, 2014)\":      wandb.Image(f\"../../analysis/means/means_2014_(H).png\"),\n",
    "           \"State (Mean, 2015)\":      wandb.Image(f\"../../analysis/means/means_2015_(H).png\")})\n",
    "\n",
    "# Closing the wandb session\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9aa051",
   "metadata": {},
   "source": [
    "<hr style=\"color:#5A7D9F;\">\n",
    "<p align=\"center\">\n",
    "    <b style=\"font-size:2vw; color:#5A7D9F; font-weight:bold;\">\n",
    "    <center>Playground</center>\n",
    "    </b>\n",
    "</p>\n",
    "<hr style=\"color:#5A7D9F;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6356ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------\n",
    "#    Parameters\n",
    "# -----------------\n",
    "#\n",
    "# Time window\n",
    "month_starting = 1\n",
    "month_ending   = 2\n",
    "year_starting  = 1980\n",
    "year_ending    = 1980\n",
    "\n",
    "# ------------------\n",
    "#  Loading the data\n",
    "# ------------------\n",
    "# Loading the different datasets\n",
    "BSD_dataset = BlackSea_Dataset(year_start  = year_starting,\n",
    "                               year_end    = year_ending,\n",
    "                               month_start = month_starting,\n",
    "                               month_end   = month_ending)\n",
    "\n",
    "# Loading the days ID (used to give temporal information to the model)\n",
    "days_ID = BSD_dataset.get_days()\n",
    "\n",
    "# Loading the different inputs\n",
    "data_temperature   = BSD_dataset.get_data(variable = \"temperature\")\n",
    "data_salinity      = BSD_dataset.get_data(variable = \"salinity\")\n",
    "data_chlorophyll   = BSD_dataset.get_data(variable = \"chlorophyll\")\n",
    "data_kshort        = BSD_dataset.get_data(variable = \"kshort\")\n",
    "data_klong         = BSD_dataset.get_data(variable = \"klong\")\n",
    "\n",
    "# Loading the output\n",
    "data_oxygen = BSD_dataset.get_data(variable = \"oxygen\")\n",
    "\n",
    "# Loading spatial information\n",
    "bathy = BSD_dataset.get_depth(unit = \"meter\")\n",
    "mesh  = BSD_dataset.get_mesh(x = 256, y = 576)\n",
    "\n",
    "# Hypoxia treshold\n",
    "hypox_tresh = xarray.open_dataset(BSD_dataset.paths[0])[\"HYPON\"].data.item()\n",
    "\n",
    "# Loading the black sea masks\n",
    "bs_mask             = BSD_dataset.get_mask(continental_shelf = False)\n",
    "bs_mask_with_depth  = BSD_dataset.get_mask(continental_shelf = True)\n",
    "bs_mask_complete    = get_complete_mask(data_oxygen, hypox_tresh, bs_mask_with_depth)\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "#  Preprocessing the data\n",
    "# -----------------------\n",
    "#\n",
    "# Creation of the dataloader\n",
    "BSD_loader = BlackSea_Dataloader(x = [data_temperature],\n",
    "                                 y = data_oxygen,\n",
    "                                 t = days_ID,\n",
    "                              mesh = mesh,\n",
    "                              mask = bs_mask,\n",
    "                   mask_with_depth = bs_mask_with_depth,\n",
    "                        bathymetry = bathy,\n",
    "                        window_inp = 1,\n",
    "                        window_out = 1,\n",
    "                    window_transfo = 1,\n",
    "                              mode = \"regression\",\n",
    "                  hypoxia_treshold = hypox_tresh)\n",
    "\n",
    "# Retrieving the datasets\n",
    "ds_train      = BSD_loader.get_dataloader(\"train\")\n",
    "ds_validation = BSD_loader.get_dataloader(\"validation\")\n",
    "ds_test       = BSD_loader.get_dataloader(\"test\")\n",
    "\n",
    "# Extracting the normalized oxygen treshold value\n",
    "norm_oxy = BSD_loader.get_normalized_deoxygenation_treshold()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c985c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, t, y in ds_train:\n",
    "\n",
    "    print(x.shape)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95e2d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------\n",
    "#\n",
    "#        |\n",
    "#       / \\\n",
    "#      / _ \\                  ESA - PROJECT\n",
    "#     |.o '.|\n",
    "#     |'._.'|          BLACK SEA DEOXYGENATION EMULATOR\n",
    "#     |     |\n",
    "#   ,'|  |  |`.             BY VICTOR MANGELEER\n",
    "#  /  |  |  |  \\\n",
    "#  |,-'--|--'-.|                2023-2024\n",
    "#\n",
    "#\n",
    "# -------------------------------------------------------\n",
    "#\n",
    "# Documentation\n",
    "# -------------\n",
    "# A neural network definition to be used as temporal encoder\n",
    "#\n",
    "# Pytorch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class ENCODER(nn.Sequential):\n",
    "    r\"\"\"A neural network used to encode the temporal information of the data and return weights for the input data\"\"\"\n",
    "\n",
    "    def __init__(self, input_size : int):\n",
    "        super(ENCODER, self).__init__()\n",
    "\n",
    "        # Defining the layers\n",
    "        self.linear_in       = nn.Linear(input_size, 256)\n",
    "        self.linear_middle_1 = nn.Linear(256,        256)\n",
    "        self.linear_middle_2 = nn.Linear(256,        128)\n",
    "        self.linear_middle_3 = nn.Linear(128,         64)\n",
    "        self.linear_middle_4 = nn.Linear(64,          32)\n",
    "        self.linear_out      = nn.Linear(32,           1)\n",
    "\n",
    "        # Defining the activation functions\n",
    "        self.activation = nn.GELU()\n",
    "\n",
    "        # Defining the softmax function, i.e. (t, values, day) to (t, values, 1) then (t, weights, 1)\n",
    "        self.softmax = nn.Softmax(dim = 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Applying the layers\n",
    "        x = self.activation(self.linear_in(x))\n",
    "        x = self.activation(self.linear_middle_1(x))\n",
    "        x = self.activation(self.linear_middle_2(x))\n",
    "        x = self.activation(self.linear_middle_3(x))\n",
    "        x = self.activation(self.linear_middle_4(x))\n",
    "        x = self.linear_out(x)\n",
    "\n",
    "        # Applying the softmax function\n",
    "        return self.softmax(x)\n",
    "\n",
    "    def count_parameters(self,):\n",
    "        r\"\"\"Determines the number of trainable parameters in the model\"\"\"\n",
    "        return int(sum(p.numel() for p in self.parameters() if p.requires_grad))\n",
    "\n",
    "class FCNN(nn.Sequential):\n",
    "    r\"\"\"A fully convolutional neural network\"\"\"\n",
    "\n",
    "    def __init__(self, problem: str, inputs: int, outputs: int, window_transformation: int = 1, kernel_size : int = 3, scaling : int = 1):\n",
    "        super(FCNN, self).__init__()\n",
    "\n",
    "        # Initialization\n",
    "        self.n_in    = inputs\n",
    "        self.problem = problem\n",
    "        self.padding = kernel_size // 2\n",
    "\n",
    "        # Number of output channels, i.e. times 2 because either mean and std for regression or both classes for classification\n",
    "        self.n_out   = outputs * 2\n",
    "\n",
    "        # ------ Architecture ------\n",
    "        #\n",
    "        # Temporal Encoder\n",
    "        self.block_encoder = ENCODER(window_transformation)\n",
    "\n",
    "        # Main Layers\n",
    "        self.conv_init           = nn.Conv2d(self.n_in    , 256 * scaling, kernel_size, padding = self.padding)\n",
    "        self.conv_intermediate_1 = nn.Conv2d(256 * scaling, 128 * scaling, kernel_size, padding = self.padding)\n",
    "        self.conv_intermediate_2 = nn.Conv2d(128 * scaling,  64 * scaling, kernel_size, padding = self.padding)\n",
    "        self.conv_intermediate_3 = nn.Conv2d( 64 * scaling,  32 * scaling, kernel_size, padding = self.padding)\n",
    "        self.conv_final          = nn.Conv2d( 32 * scaling,    self.n_out, kernel_size, padding = self.padding)\n",
    "\n",
    "        # Activation function\n",
    "        self.activation = nn.GELU()\n",
    "\n",
    "        # Normalization\n",
    "        self.normalization_init           = nn.BatchNorm2d(self.conv_init.out_channels)\n",
    "        self.normalization_intermediate_1 = nn.BatchNorm2d(self.conv_intermediate_1.out_channels)\n",
    "        self.normalization_intermediate_2 = nn.BatchNorm2d(self.conv_intermediate_2.out_channels)\n",
    "        self.normalization_intermediate_3 = nn.BatchNorm2d(self.conv_intermediate_3.out_channels)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "\n",
    "        # Retrieiving dimensions (Ease of comprehension)\n",
    "        samples, days, values, variables, x_res, y_res = x.shape\n",
    "\n",
    "        # ----- Encoding Time -----\n",
    "        #\n",
    "        # Applying the encoder\n",
    "        weights = torch.squeeze(self.block_encoder(t), dim = -1)\n",
    "\n",
    "        # Applying the weights (except to mesh (dim = 2) and bathymetry (dim = 3))\n",
    "        for sample in range(samples):\n",
    "            for value in range(days):\n",
    "                x[:, value, :, :-3] *= weights[sample, value]\n",
    "\n",
    "        # Reshaping\n",
    "        x = x.reshape(samples, days * values * variables, x_res, y_res)\n",
    "\n",
    "        # ----- Fully Convolutionnal -----\n",
    "        #\n",
    "        x = self.normalization_init(self.activation(self.conv_init(x)))\n",
    "        x = self.normalization_intermediate_1(self.activation(self.conv_intermediate_1(x)))\n",
    "        x = self.normalization_intermediate_2(self.activation(self.conv_intermediate_2(x)))\n",
    "        x = self.normalization_intermediate_3(self.activation(self.conv_intermediate_3(x)))\n",
    "        x = self.conv_final(x)\n",
    "\n",
    "        # Retrieiving dimensions (Ease of comprehension)\n",
    "        b, c, x_res, y_res = x.shape\n",
    "\n",
    "        # Reshaping the output, i.e. (samples, days, values, x, y)\n",
    "        return x.reshape(b, self.n_out // 2, 2, x_res, y_res)\n",
    "\n",
    "    def count_parameters(self,):\n",
    "        r\"\"\"Determines the number of trainable parameters in the model\"\"\"\n",
    "        return int(sum(p.numel() for p in self.parameters() if p.requires_grad))\n",
    "\n",
    "class AVERAGE(nn.Sequential):\n",
    "    r\"\"\"A 'neural network' that predicts the pixel temporal average (should be used a baseline)\"\"\"\n",
    "\n",
    "    def __init__(self, data_output : np.array, device : str, kwargs : dict):\n",
    "        super(AVERAGE, self).__init__()\n",
    "\n",
    "        # Extracting information\n",
    "        dataset_size     = [0.6, 0.3]\n",
    "        problem          = \"regression\"\n",
    "        hypoxia_treshold = 0.1\n",
    "\n",
    "        # Retrieiving dimensions\n",
    "        t, x, y = data_output.shape\n",
    "\n",
    "        # Number of training samples\n",
    "        train_samples = int(t * dataset_size[0])\n",
    "\n",
    "        # ----- Regression ------\n",
    "        if problem == \"regression\":\n",
    "\n",
    "            # Determine the minimum and maximum values of the data\n",
    "            min_value = np.nanmin(data_output)\n",
    "            max_value = np.nanmax(data_output)\n",
    "\n",
    "            # Determining the minimum and maximum values\n",
    "            min_value = np.nanmin(data_output)\n",
    "            max_value = np.nanmax(data_output)\n",
    "\n",
    "            # Shift the data to ensure minimum value is 0\n",
    "            shifted_data = data_output - min_value\n",
    "\n",
    "            # Normalizing the data\n",
    "            normalized_data = shifted_data / (max_value - min_value)\n",
    "\n",
    "            # Predicting the average and log of variance\n",
    "            average_output = torch.mean(torch.from_numpy(normalized_data[: train_samples, :, :]), dim = 0)\n",
    "            std_output     = torch.log(torch.var(torch.from_numpy(normalized_data[: train_samples, :, :]), dim = 0))\n",
    "\n",
    "            # Stacking\n",
    "            average_output = torch.stack([average_output, std_output])\n",
    "\n",
    "        # ----- Classification ------\n",
    "        else:\n",
    "\n",
    "            # Converting to classification\n",
    "            average_output = torch.from_numpy((data_output[: train_samples, :, :] < hypoxia_treshold) * 1)\n",
    "\n",
    "            # Summing over time, i.e. if total number of hypoxic days is greater than 50% of the time, then it is hypoxic\n",
    "            average_output = (torch.sum(average_output, dim = 0) > train_samples // 2) * 1\n",
    "\n",
    "            # Conversion to \"probabilities\", i.e. (t, x, y) to (t, c, x, y) with c = 0 no hypoxia, c = 1 hypoxia\n",
    "            average_output = torch.stack([(average_output == 0) * 1, average_output]).float()\n",
    "\n",
    "        # Storing information\n",
    "        self.outputs = 1\n",
    "        self.bs      = 64\n",
    "        self.average = self.process(average_output)\n",
    "        self.device  = \"cuda\"\n",
    "\n",
    "        # Dummy feature (It plays no role whatsoever, it is just a placeholder to make the model work with the trainer)\n",
    "        self.layer = nn.Conv2d(1, 1, 1)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        return to_device(self.average[:x.shape[0]], self.device)\n",
    "\n",
    "    def process(self, x : torch.Tensor):\n",
    "        r\"\"\"Used to format the output to the correct shape\"\"\"\n",
    "\n",
    "        # Adding number of forecasted days\n",
    "        x = torch.unsqueeze(x, dim = 0) if self.outputs == 1 else \\\n",
    "            torch.stack([x for i in range(self.outputs)], dim = 0)\n",
    "\n",
    "        # Adding batch size\n",
    "        return torch.stack([x for i in range(self.bs)], dim = 0)\n",
    "\n",
    "    def count_parameters(self,):\n",
    "        r\"\"\"Determines the number of trainable parameters in the model\"\"\"\n",
    "        return int(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76799521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network stuff\n",
    "neural_network = FCNN(problem = \"regression\",\n",
    "                      inputs  = 4,\n",
    "                      outputs = 1,\n",
    "                      window_transformation = 1,\n",
    "                      kernel_size = 1,\n",
    "                      scaling = 1)\n",
    "\n",
    "\n",
    "#neural_network = AVERAGE(data_oxygen, \"cuda\", {})\n",
    "\n",
    "\n",
    "optimizer      = optim.Adam(neural_network.parameters(), lr = 0.001)\n",
    "\n",
    "# Check if a GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "neural_network.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86506486",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ratios_plot(data_oxygen, hypox_tresh, bs_mask_with_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa9a122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "show = False\n",
    "\n",
    "for epoch in range(1):\n",
    "\n",
    "    metrics_tool = BlackSea_Metrics(mode = \"regression\",\n",
    "                                    mask = bs_mask_with_depth,\n",
    "                           mask_complete = bs_mask_complete,\n",
    "                                treshold = norm_oxy,\n",
    "                       number_of_samples = BSD_loader.get_number_of_samples(\"validation\"))\n",
    "\n",
    "    for x, t, y in ds_train:\n",
    "\n",
    "        x, t, y = x.to(device), t.to(device), y.to(device)\n",
    "        prediction = neural_network(x, t)\n",
    "        loss_training = compute_loss(y_pred = prediction, y_true = y,mask = bs_mask_with_depth, problem = \"regression\", device = \"cpu\", kwargs = {})\n",
    "        print(f\"E{epoch} - Loss (Training):\", loss_training.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss_training.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Cleaning\n",
    "        del x, t, y, prediction, loss_training\n",
    "        torch.cuda.empty_cache()\n",
    "        break\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # Stores all the predictions for the metrics (plots)\n",
    "        prediction_all = None\n",
    "\n",
    "        for x, t, y in ds_validation:\n",
    "\n",
    "            # Making prediction\n",
    "            x, t, y = x.to(device), t.to(device), y.to(device)\n",
    "            prediction = neural_network(x, t)\n",
    "            loss_validation = compute_loss(y_pred = prediction, y_true = y, mask = bs_mask_with_depth, problem = \"regression\", device = \"cpu\", kwargs = {})\n",
    "            print(f\"E{epoch} - Loss (Validation):\", loss_validation.item())\n",
    "            x, t, y, prediction = x.to(\"cpu\"), t.to(\"cpu\"), y.to(\"cpu\"), prediction.to(\"cpu\")\n",
    "\n",
    "            \"\"\"\n",
    "            # Plotting mean against ground truth in a subplot\n",
    "            if show:\n",
    "\n",
    "                # Highlighting hypoxic areas\n",
    "                y_hyp = ( y < norm_oxy ) * 1.0\n",
    "                p_hyp = ( prediction < norm_oxy ) * 1.0\n",
    "\n",
    "                # Hiding non-obserable areas\n",
    "                p_hyp[:,:,:, y[0, 0, 0] == -1] = torch.nan\n",
    "                y_hyp[:,:,:, y[0, 0, 0] == -1] = torch.nan\n",
    "\n",
    "\n",
    "                plt.figure(figsize = (20, 20))\n",
    "                plt.subplot(1, 3, 1)\n",
    "                plt.imshow(torch.flipud(y_hyp[0, 0, 0]))\n",
    "                plt.subplot(1, 3, 2)\n",
    "                plt.imshow(torch.flipud(p_hyp[0, 0, 0]))\n",
    "                plt.subplot(1, 3, 3)\n",
    "                plt.imshow(torch.flipud(y_hyp[0, 0, 0]) - torch.flipud(p_hyp[0, 0, 0]))\n",
    "                plt.setp(plt.gcf().get_axes(), xticks = [], yticks = [])\n",
    "                plt.subplot(1, 3, 1).set_title(\"Ground Truth\", fontsize = 6)\n",
    "                plt.subplot(1, 3, 2).set_title(\"Prediction\", fontsize = 6)\n",
    "                plt.subplot(1, 3, 3).set_title(\"Difference\", fontsize = 6)\n",
    "                plt.show()\n",
    "\n",
    "                prediction[:,:,:, y[0,0,0,:,:] == -1] = torch.nan\n",
    "                y[:,:,:, y[0,0,0,:,:] == -1] = torch.nan\n",
    "\n",
    "                plt.figure(figsize = (20, 20))\n",
    "                plt.subplot(1, 3, 1)\n",
    "                plt.imshow(torch.flipud(y[0, 0, 0]),  vmin = 0, vmax = 1)\n",
    "                plt.subplot(1, 3, 2)\n",
    "                plt.imshow(torch.flipud(prediction[0, 0, 0]), vmin = 0, vmax = 1)\n",
    "                plt.subplot(1, 3, 3)\n",
    "                plt.imshow(torch.flipud(torch.exp(prediction[0, 0, 1]/2)), vmin = 0, vmax = 1)\n",
    "                plt.setp(plt.gcf().get_axes(), xticks = [], yticks = [])\n",
    "                plt.subplot(1, 3, 1).set_title(\"Ground Truth\", fontsize = 6)\n",
    "                plt.subplot(1, 3, 2).set_title(\"Prediction (Mean)\", fontsize = 6)\n",
    "                plt.subplot(1, 3, 3).set_title(\"Prediction (Std)\", fontsize = 6)\n",
    "                plt.show()\n",
    "\n",
    "            \"\"\"\n",
    "\n",
    "            # Concatenating all the predictions\n",
    "            prediction_all = torch.cat((prediction_all, prediction), dim = 0) if prediction_all is not None else prediction\n",
    "\n",
    "            del x, t, y, prediction\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "    # Sampling random data for comparison\n",
    "    # Metrics\n",
    "    y_vall_all = torch.from_numpy(BSD_loader.y_validation)\n",
    "    #metrics_tool.compute_metrics(y_pred = prediction_all, y_true = y_vall_all)\n",
    "    metrics_tool.compute_plots_comparison_regression(y_pred = prediction_all, y_true = y_vall_all)\n",
    "\n",
    "\n",
    "    #metrics_tool.compute_plots(  y_pred = prediction_all, y_true = y_vall_all)\n",
    "\n",
    "    # Getting the results\n",
    "    \"\"\"\n",
    "    if show:\n",
    "        results, results_name = metrics_tool.get_results()\n",
    "        for r, n in zip(results[0], results_name):\n",
    "            print(n, \" : \", r)\n",
    "        print(\"\\n\")\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e4ea30",
   "metadata": {},
   "outputs": [],
   "source": [
    "BSD_loader.get_number_of_samples(\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3717feca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "a = np.random.randint(0, prediction_all.shape[0] - 1, 3)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79aa1d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in a:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3213e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esa",
   "language": "python",
   "name": "esa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
