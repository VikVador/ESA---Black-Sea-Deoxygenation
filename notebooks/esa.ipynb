{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a999b204-3b95-404f-93a1-90b1bda33abb",
   "metadata": {},
   "source": [
    "<img src=\"../assets/header_notebook.png\" />\n",
    "<hr style=\"color:#5A7D9F;\">\n",
    "<p align=\"center\">\n",
    "    <b style=\"font-size:2vw; color:#5A7D9F; font-weight:bold;\">\n",
    "    <center>ESA - Black Sea Deoxygenation Emulator</center>\n",
    "    </b>\n",
    "</p>\n",
    "<hr style=\"color:#5A7D9F;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b55dcb8-9067-463c-b876-2e45565d6240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------\n",
    "# Librairies\n",
    "# ----------\n",
    "import os\n",
    "import sys\n",
    "import xarray\n",
    "import random\n",
    "import dawgz\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Plots\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "# Dawgz (jobs //)\n",
    "from dawgz import job, after, ensure, schedule\n",
    "\n",
    "# -------------------\n",
    "# Librairies (Custom)\n",
    "# -------------------\n",
    "# Adding path to source folder to load custom modules\n",
    "sys.path.insert(1, '../src/')\n",
    "sys.path.insert(1, '../scripts/')\n",
    "\n",
    "# Loading libraries\n",
    "from dataset              import BlackSea_Dataset\n",
    "from dataset_evolution    import BlackSea_Dataset_Evolution\n",
    "from dataset_distribution import BlackSea_Dataset_Distribution\n",
    "\n",
    "# -------\n",
    "# Jupyter\n",
    "# -------\n",
    "%matplotlib inline\n",
    "plt.rcParams.update({'font.size': 13})\n",
    "\n",
    "# Making sure modules are reloaded when modified\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Moving to the .py directory\n",
    "%cd ../src/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9c007b-1cab-4407-a5f0-cbcdd836a569",
   "metadata": {},
   "source": [
    "<hr style=\"color:#5A7D9F;\">\n",
    "<p align=\"center\">\n",
    "    <b style=\"font-size:2vw; color:#5A7D9F; font-weight:bold;\">\n",
    "    <center>Scripts</center>\n",
    "    </b>\n",
    "</p>\n",
    "<hr style=\"color:#5A7D9F;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e827c7-75f2-414b-8af6-0a83575dfc4e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Analyzing the data (1):\n",
    "%run script_distribution.py --start_year        0 \\\n",
    "                            --end_year          0 \\\n",
    "                            --start_month       1 \\\n",
    "                            --end_month         2 \\\n",
    "                            --dawgz         False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634c159c-2635-4cc2-ae57-cdaaf4c9541f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Analyzing the data (2):\n",
    "%run script_evolution.py --start_year        0 \\\n",
    "                         --end_year          0 \\\n",
    "                         --start_month       1 \\\n",
    "                         --end_month         2 \\\n",
    "                         --dawgz         False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7aae61-7785-4d43-abbb-c3ad280f50e6",
   "metadata": {},
   "source": [
    "<hr style=\"color:#5A7D9F;\">\n",
    "<p align=\"center\">\n",
    "    <b style=\"font-size:2vw; color:#5A7D9F; font-weight:bold;\">\n",
    "    <center>Playground</center>\n",
    "    </b>\n",
    "</p>\n",
    "<hr style=\"color:#5A7D9F;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77694f02-1f69-444b-b479-89da86516e77",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# -----------------\n",
    "#    Parameters\n",
    "# -----------------\n",
    "#\n",
    "# Dataset time window\n",
    "month_starting = 1\n",
    "month_ending   = 2\n",
    "year_starting  = 0\n",
    "year_ending    = 0\n",
    "\n",
    "# Maximum depth observed for oxygen, what is left is masked [m]. To observe only the continental shelf set it to ~120m\n",
    "depth_max_oxygen = None\n",
    "\n",
    "# ------------------\n",
    "#  Loading the data\n",
    "# ------------------\n",
    "# Dataset handlers !\n",
    "Dataset_physical = BlackSea_Dataset(year_start = year_starting, year_end = year_ending, month_start = month_starting,  month_end = month_ending, variable = \"grid_T\")\n",
    "Dataset_bio      = BlackSea_Dataset(year_start = year_starting, year_end = year_ending, month_start = month_starting,  month_end = month_ending, variable = \"ptrc_T\")\n",
    "\n",
    "# Loading the different field values\n",
    "data_temperature   = Dataset_physical.get_temperature()\n",
    "data_salinity      = Dataset_physical.get_salinity()\n",
    "data_oxygen        = Dataset_bio.get_oxygen_bottom(depth = depth_max_oxygen)\n",
    "data_chlorophyll   = Dataset_bio.get_chlorophyll()\n",
    "data_kshort        = Dataset_bio.get_light_attenuation_coefficient_short_waves()\n",
    "data_klong         = Dataset_bio.get_light_attenuation_coefficient_long_waves()\n",
    "\n",
    "# Loading the black sea mask\n",
    "BS_mask = Dataset_physical.get_blacksea_mask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f07499-5cdb-410f-a68e-0b37d2b9e3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class BlackSea_Dataloader(Dataset):\n",
    "    r\"\"\"A simple dataloader for Black Sea dataset\"\"\"\n",
    "\n",
    "    def __init__(self, x: list, y: np.array, bs_mask, mode: str, resolution: int,  window: int = 1, datasets_size = [0.5, 0.3], seed: int = 42):\n",
    "\n",
    "        # Concatenation of the inputs and output (t, x, y) into (t, variable, x, y), i.e. variable = 0 is the target\n",
    "        x = np.stack([y] + x, axis = 1)\n",
    "\n",
    "        # Masking the land (NaNs) with a fixed value (-10)\n",
    "        x[:, :, bs_mask == 0] = -1\n",
    "\n",
    "        # Removing dimensions to be multiple of 2 for ease of use (original shape:(258, 578), target:(256, 576) obtained by removing last 2-rows/bottoms)\n",
    "        x = x[:, :, :-2, :-2]\n",
    "\n",
    "        # Current shape of the input (1)\n",
    "        t, v, x_res, y_res = x.shape\n",
    "\n",
    "        # Security\n",
    "        assert mode in [\"spatial\", \"temporal\"], f\"ERROR (BlackSea_Dataloader) Mode must be either 'spatial', 'temporal' ({mode})\"\n",
    "        assert resolution % 2 == 0,             f\"ERROR (BlackSea_Dataloader) Resolution must be a multiple of 2 ({resolution})\"\n",
    "        assert resolution < x_res/2,            f\"ERROR (BlackSea_Dataloader) Resolution must be smaller than half the input resolution ({resolution} < {x_res/2})\"\n",
    "        assert window <= int(t/3 - 1),          f\"ERROR (BlackSea_Dataloader) Window must be smaller than a third of the input time resolution ({window} < {int(t/3 - 1)})\"\n",
    "\n",
    "        # Concatenation of the inputs (t, x, y) into (t, variable, x, y)\n",
    "        x = np.stack(x, axis = 0)\n",
    "\n",
    "        # Removing dimensions to be multiple of 2 for ease of use (original shape:(258, 578), target:(256, 576) obtained by removing last 2-rows/bottoms)\n",
    "        x = x[:, :, :-2, :-2]\n",
    "\n",
    "        # Current shape of the input (2)\n",
    "        t, v, x_res, y_res = x.shape\n",
    "\n",
    "        # Number of patches along the x-, y- dimensions and total number of possible patches\n",
    "        nb_patches_x, nb_patches_y, total_patches = int(x_res/resolution), int(y_res/resolution), int(x_res/resolution) * int(y_res/resolution)\n",
    "\n",
    "        # Extracting patches from the input of of a given resolution\n",
    "        x = [x[:, :, i * resolution : (i + 1) * resolution, j * resolution : (j + 1) * resolution] for i in range(nb_patches_x) for j in range(nb_patches_y)]\n",
    "\n",
    "        # Concatenation of the inputs (t, variable, x, y) into (t, variables, number of patches, resolution, resolution)\n",
    "        x = np.stack(x, axis = 2)\n",
    "\n",
    "        # Separation of the x and y data (to avoid a mess with timeseries)\n",
    "        y = x[:, 0,  :, :, :]\n",
    "        x = x[:, 1:, :, :, :]\n",
    "\n",
    "        # Input - Creation of the time series, i.e. (index, variable(s)_{t, t + window}, number of patches, resolution, resolution)\n",
    "        x = np.stack([x[i : i + window, :, :, :, :] for i in range(t - window)], axis = 0).reshape(t - window, (v - 1) * window, total_patches, resolution, resolution)\n",
    "\n",
    "        # Output - Creation of the output pair (only the last value of time series is important), i.e. (index, variable(s)_{t + window}, number of patches, resolution, resolution)\n",
    "        y = np.stack([y[i + window,  :, :, :]    for i in range(t - window)], axis = 0)\n",
    "\n",
    "        # Used to merge timeseries and patches dimensions,\n",
    "        def merge_timeseries_and_patches(data: np.array):\n",
    "\n",
    "            # If dimensions is 5, the input has been given\n",
    "            if len(data.shape) == 5:\n",
    "\n",
    "                # Swaping axes (needed to concatenate along patch dimensino during reshaping), i.e. (t, v, p, r, r) to (t, p, v, r, r)\n",
    "                data = np.swapaxes(data, 1, 2)\n",
    "\n",
    "                # Retrieving all the dimensions for reshaping\n",
    "                t, p, v, res_x, res_y = data.shape\n",
    "\n",
    "                # Merging timeseries and patches dimensions\n",
    "                return data.reshape(t * p, v, res_x, res_y)\n",
    "\n",
    "            # If dimension is 4, the output has been given\n",
    "            else:\n",
    "\n",
    "                # Retrieving all the dimensions for reshaping\n",
    "                t, p, res_x, res_y = data.shape\n",
    "\n",
    "                # Merging timeseries and patches dimensions\n",
    "                data = data.reshape(t * p, res_x, res_y)\n",
    "\n",
    "                # Adding a dummy dimensions to simulate channels\n",
    "                return np.expand_dims(data, axis = 1)\n",
    "\n",
    "        # ------------------------------------------------\n",
    "        #                   TEMPORAL MODE\n",
    "        # ------------------------------------------------\n",
    "        if mode == \"temporal\":\n",
    "\n",
    "            # Computing size of the training, validation and test sets\n",
    "            training_size, validation_size = int(t * datasets_size[0]), int(t * datasets_size[1])\n",
    "\n",
    "            # Splitting the dataset into training, validation and test sets while not taking overlapping timeseries\n",
    "            x_train, x_validation, x_test = x[:training_size - window, :, :, :, :], x[training_size:training_size + validation_size - window, :, :, :, :], x[training_size + validation_size:, :, :, :, :]\n",
    "            y_train, y_validation, y_test = y[:training_size - window,    :, :, :], y[training_size:training_size + validation_size - window,    :, :, :], y[training_size + validation_size:,    :, :, :]\n",
    "\n",
    "            # It is merging time !\n",
    "            x_train      = merge_timeseries_and_patches(x_train)\n",
    "            x_validation = merge_timeseries_and_patches(x_validation)\n",
    "            x_test       = merge_timeseries_and_patches(x_test)\n",
    "            y_train      = merge_timeseries_and_patches(y_train)\n",
    "            y_validation = merge_timeseries_and_patches(y_validation)\n",
    "            y_test       = merge_timeseries_and_patches(y_test)\n",
    "\n",
    "        # ------------------------------------------------\n",
    "        #                   SPATIAL MODE\n",
    "        # ------------------------------------------------\n",
    "        if mode == \"spatial\":\n",
    "\n",
    "            # Computing size of the training, validation and test sets\n",
    "            training_size, validation_size = int(total_patches * datasets_size[0]), int(total_patches * datasets_size[1])\n",
    "\n",
    "            # Used to randomly permute the patches !\n",
    "            rand_patches = np.random.permutation(total_patches)\n",
    "\n",
    "            # Randomly shuffling along the patches axis\n",
    "            x = x[:, :, rand_patches, :, :]\n",
    "            y = y[:,    rand_patches, :, :]\n",
    "\n",
    "            # Splitting the dataset into training, validation and test sets\n",
    "            x_train, x_validation, x_test = x[:, :, :training_size, :, :], x[:, :, training_size:training_size + validation_size, :, :], x[:, :, training_size + validation_size:, :, :]\n",
    "            y_train, y_validation, y_test = y[:,    :training_size, :, :], y[:,    training_size:training_size + validation_size, :, :], y[:,    training_size + validation_size:, :, :]\n",
    "\n",
    "            # It is merging time !\n",
    "            x_train      = merge_timeseries_and_patches(x_train)\n",
    "            x_validation = merge_timeseries_and_patches(x_validation)\n",
    "            x_test       = merge_timeseries_and_patches(x_test)\n",
    "            y_train      = merge_timeseries_and_patches(y_train)\n",
    "            y_validation = merge_timeseries_and_patches(y_validation)\n",
    "            y_test       = merge_timeseries_and_patches(y_test)\n",
    "\n",
    "        # ------------------------------------------------\n",
    "        #                      PROCESSING\n",
    "        # ------------------------------------------------\n",
    "        # Used to replace the NaNs, standardize the values in the ocean and add the black sea mask on the channel dimension\n",
    "        def process_data(data: np.array, black_sea_mask: np.array):\n",
    "\n",
    "            # Creation of the mask\n",
    "            mask = data[:, :] != -1\n",
    "\n",
    "            # Standardizing the values in the ocean (i.e. not land)\n",
    "            data[mask] = (data[mask] - np.mean(data[mask])) / np.std(data[mask])\n",
    "\n",
    "            # Replacing NaNs by -1\n",
    "            data[np.isnan(data)] = -1\n",
    "\n",
    "            return data\n",
    "\n",
    "        # Replace Nans (land) by -1\n",
    "        self.x_train      = process_data(x_train, bs_mask)\n",
    "        self.x_validation = process_data(x_validation, bs_mask)\n",
    "        self.x_test       = process_data(x_test, bs_mask)\n",
    "        self.y_train      = process_data(y_train, bs_mask)\n",
    "        self.y_validation = process_data(y_validation, bs_mask)\n",
    "        self.y_test       = process_data(y_test, bs_mask)\n",
    "\n",
    "    def __getitem__(self, index, train = True):\n",
    "        return (self.x_train[index, :, :, :], self.y_train[index, :, :, :]) if train else (self.x_validation[index, :, :, :], self.y_validation[index, :, :, :])\n",
    "\n",
    "    def __len__(self, train = True):\n",
    "        return self.x_train.shape[0] if train else self.x_validation.shape[0]\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "BS_dataset = BlackSea_Dataloader(x = [data_temperature, data_chlorophyll], y = data_oxygen, bs_mask = BS_mask, mode = \"spatial\", resolution = 64, window = 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eef6589",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyCNN(nn.Sequential):\n",
    "    def __init__(self, inputs, targets):\n",
    "\n",
    "        # Dimension of input and output data\n",
    "        n_in  = inputs\n",
    "        n_out = targets\n",
    "        padding_5 = 2\n",
    "        padding_3 = 1\n",
    "\n",
    "        #-----------------------------------------------------------------------------------\n",
    "        #                                   Architecture\n",
    "        #-----------------------------------------------------------------------------------\n",
    "        block1 = self._make_subblock(nn.Conv2d(n_in, 256, 5, padding = padding_5))\n",
    "        block2 = self._make_subblock(nn.Conv2d(256,  128, 5, padding = padding_5))\n",
    "        block3 = self._make_subblock(nn.Conv2d(128,   32, 3, padding = padding_3))\n",
    "        block4 = self._make_subblock(nn.Conv2d(32,    32, 3, padding = padding_3))\n",
    "        block5 = self._make_subblock(nn.Conv2d(32,    32, 3, padding = padding_3))\n",
    "        block6 = self._make_subblock(nn.Conv2d(32,    32, 3, padding = padding_3))\n",
    "        block7 = self._make_subblock(nn.Conv2d(32,    32, 3, padding = padding_3))\n",
    "        conv8  =                     nn.Conv2d(32, n_out, 3, padding = padding_3)\n",
    "\n",
    "        # Combining everything together\n",
    "        super().__init__(*block1, *block2, *block3, *block4, *block5, *block6, *block7, conv8)\n",
    "\n",
    "    def _make_subblock(self, conv):\n",
    "        return [conv, nn.ReLU(), nn.BatchNorm2d(conv.out_channels)]\n",
    "\n",
    "    #-----------------------------------------------------------------------------------\n",
    "    #                                     Forward\n",
    "    #-----------------------------------------------------------------------------------\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Final prediction\n",
    "        return super().forward(x)\n",
    "\n",
    "    def count_parameters(self,):\n",
    "        print(\"Model parameters  =\", sum(p.numel() for p in self.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bace229f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wind = 1\n",
    "# ----------------------\n",
    "#\n",
    "# ----------------------\n",
    "bs_load = BlackSea_Dataloader(x = [data_temperature, data_chlorophyll, data_salinity, data_kshort, data_klong], y = data_oxygen, bs_mask = BS_mask, mode = \"temporal\", resolution = 64, window = wind)\n",
    "bs_loader = DataLoader(bs_load, batch_size = 64)\n",
    "\n",
    "## NN\n",
    "FCNN = FullyCNN(wind * 5, 1)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(FCNN.parameters(), lr = 0.01)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode = 'min', factor = 0.5, patience = 5, threshold = 1e-2)\n",
    "\n",
    "# Used to compute the average loss value over the all epoch\n",
    "train_losses = []\n",
    "validation_losses = []\n",
    "\n",
    "# Going through epochs\n",
    "for epoch in range(15):\n",
    "    for x, y in bs_loader:\n",
    "\n",
    "        pred = FCNN.forward(x)\n",
    "\n",
    "        # Computing the loss\n",
    "        loss = criterion(pred, y)\n",
    "\n",
    "        # Adding the loss\n",
    "        train_losses.append(loss.detach().item())\n",
    "\n",
    "        # Reseting the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Optimizing the parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        print(F\"Train{epoch}:\", train_losses[-1])\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(train_losses)\n",
    "plt.plot(validation_losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
