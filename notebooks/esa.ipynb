{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a999b204-3b95-404f-93a1-90b1bda33abb",
   "metadata": {},
   "source": [
    "<img src=\"../assets/header_notebook.png\" />\n",
    "<hr style=\"color:#5A7D9F;\">\n",
    "<p align=\"center\">\n",
    "    <b style=\"font-size:2vw; color:#5A7D9F; font-weight:bold;\">\n",
    "    <center>ESA - Black Sea Deoxygenation Emulator</center>\n",
    "    </b>\n",
    "</p>\n",
    "<hr style=\"color:#5A7D9F;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b55dcb8-9067-463c-b876-2e45565d6240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------\n",
    "# Librairies\n",
    "# ----------\n",
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import xarray\n",
    "import random\n",
    "import dawgz\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Dawgz (jobs //)\n",
    "from dawgz import job, schedule\n",
    "\n",
    "# -------------------\n",
    "# Librairies (Custom)\n",
    "# -------------------\n",
    "# Adding path to source folder to load custom modules\n",
    "sys.path.insert(1, '../src/debs/')\n",
    "sys.path.insert(1, '../scripts/')\n",
    "\n",
    "# Loading libraries\n",
    "from tools                import *\n",
    "from metrics              import *\n",
    "from dataset              import BlackSea_Dataset\n",
    "from dataloader           import BlackSea_Dataloader\n",
    "\n",
    "# -------\n",
    "# Jupyter\n",
    "# -------\n",
    "%matplotlib inline\n",
    "plt.rcParams.update({'font.size': 13})\n",
    "\n",
    "# Making sure modules are reloaded when modified\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Moving to the .py directory\n",
    "%cd ../src/debs/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9c007b-1cab-4407-a5f0-cbcdd836a569",
   "metadata": {},
   "source": [
    "<hr style=\"color:#5A7D9F;\">\n",
    "<p align=\"center\">\n",
    "    <b style=\"font-size:2vw; color:#5A7D9F; font-weight:bold;\">\n",
    "    <center>Scripts</center>\n",
    "    </b>\n",
    "</p>\n",
    "<hr style=\"color:#5A7D9F;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e827c7-75f2-414b-8af6-0a83575dfc4e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Analyzing the data (1):\n",
    "%run script_distribution.py --start_year        0 \\\n",
    "                            --end_year          0 \\\n",
    "                            --start_month       1 \\\n",
    "                            --end_month         2 \\\n",
    "                            --dawgz         False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634c159c-2635-4cc2-ae57-cdaaf4c9541f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Analyzing the data (2):\n",
    "%run script_evolution.py --start_year        0 \\\n",
    "                         --end_year          0 \\\n",
    "                         --start_month       1 \\\n",
    "                         --end_month         2 \\\n",
    "                         --dawgz         False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23f012a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training a neural network:\n",
    "%run script_training.py  --start_year                 0 \\\n",
    "                         --end_year                   0 \\\n",
    "                         --start_month                0 \\\n",
    "                         --end_month                  1 \\\n",
    "                         --inputs           temperature \\\n",
    "                         --problem           regression \\\n",
    "                         --windows_input              1 \\\n",
    "                         --windows_output             1 \\\n",
    "                         --architecture            FCNN \\\n",
    "                         --batch_size                64 \\\n",
    "                         --epochs                    10 \\\n",
    "                         --kernel_size                3 \\\n",
    "                         --dawgz                  False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7aae61-7785-4d43-abbb-c3ad280f50e6",
   "metadata": {},
   "source": [
    "<hr style=\"color:#5A7D9F;\">\n",
    "<p align=\"center\">\n",
    "    <b style=\"font-size:2vw; color:#5A7D9F; font-weight:bold;\">\n",
    "    <center>Playground</center>\n",
    "    </b>\n",
    "</p>\n",
    "<hr style=\"color:#5A7D9F;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77694f02-1f69-444b-b479-89da86516e77",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# -----------------\n",
    "#    Parameters\n",
    "# -----------------\n",
    "#\n",
    "# Time window\n",
    "month_starting = 1\n",
    "month_ending   = 1\n",
    "year_starting  = 0\n",
    "year_ending    = 0\n",
    "\n",
    "# Maximum depth observed for oxygen, what is left is masked [m] (Note: To observe only the continental shelf set it to ~200m).\n",
    "depth_max_oxygen = 200\n",
    "\n",
    "# ------------------\n",
    "#  Loading the data\n",
    "# ------------------\n",
    "# Loading the different datasets\n",
    "Dataset_phy = BlackSea_Dataset(year_start  = year_starting,\n",
    "                               year_end    = year_ending,\n",
    "                               month_start = month_starting,\n",
    "                               month_end   = month_ending,\n",
    "                               variable    = \"grid_T\")\n",
    "\n",
    "Dataset_bio = BlackSea_Dataset(year_start  = year_starting,\n",
    "                               year_end    = year_ending,\n",
    "                               month_start = month_starting,\n",
    "                               month_end   = month_ending,\n",
    "                               variable    = \"ptrc_T\")\n",
    "\n",
    "# Loading the different field values\n",
    "data_temperature   = Dataset_phy.get_data(variable = \"temperature\", type = \"surface\", depth = None)\n",
    "data_salinity      = Dataset_phy.get_data(variable = \"salinity\",    type = \"surface\", depth = None)\n",
    "data_chlorophyll   = Dataset_bio.get_data(variable = \"chlorophyll\", type = \"surface\", depth = None)\n",
    "data_kshort        = Dataset_bio.get_data(variable = \"k_short\",     type = \"surface\", depth = None)\n",
    "data_klong         = Dataset_bio.get_data(variable = \"k_long\",      type = \"surface\", depth = None)\n",
    "data_oxygen        = Dataset_bio.get_data(variable = \"oxygen\",      type = \"bottom\" , depth = depth_max_oxygen)\n",
    "\n",
    "# Loading the black sea mask\n",
    "bs_mask             = Dataset_phy.get_mask(depth = None)\n",
    "bs_mask_with_depth  = Dataset_phy.get_mask(depth = depth_max_oxygen)\n",
    "\n",
    "# --------------------\n",
    "#  Preparing the data\n",
    "# --------------------\n",
    "# Loading the dataloader\n",
    "BSD_loader = BlackSea_Dataloader(x = [data_temperature],\n",
    "                                 y = data_oxygen,\n",
    "                           bs_mask = bs_mask,\n",
    "                bs_mask_with_depth = bs_mask_with_depth,\n",
    "                              mode = \"regression\",\n",
    "                        window_inp = 1,\n",
    "                        window_out = 2)\n",
    "\n",
    "# Retrieving the datasets\n",
    "ds_validation = BSD_loader.get_dataloader(\"validation\")\n",
    "ds_train      = BSD_loader.get_dataloader(\"train\")\n",
    "ds_test       = BSD_loader.get_dataloader(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727401e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------\n",
    "#\n",
    "#        |\n",
    "#       / \\\n",
    "#      / _ \\                  ESA - PROJECT\n",
    "#     |.o '.|\n",
    "#     |'._.'|          BLACK SEA DEOXYGENATION EMULATOR\n",
    "#     |     |\n",
    "#   ,'|  |  |`.             BY VICTOR MANGELEER\n",
    "#  /  |  |  |  \\\n",
    "#  |,-'--|--'-.|                2023-2024\n",
    "#\n",
    "#\n",
    "# -------------------------------------------------------\n",
    "#\n",
    "# Documentation\n",
    "# -------------\n",
    "# A script to train a neural network to become a oxygen concentration forecaster in the Black Sea.\n",
    "#\n",
    "#   Dawgz = False : compute the distributions over a given time period given by the user as arguments\n",
    "#\n",
    "#   Dawgz = True  : compute the distributions over all the possible time periods\n",
    "#\n",
    "import time\n",
    "import wandb\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Custom libraries\n",
    "from dataset              import BlackSea_Dataset\n",
    "from dataloader           import BlackSea_Dataloader\n",
    "from metrics              import BlackSea_Metrics\n",
    "from neural_networks      import FCNN\n",
    "from tools                import progressBar, to_device\n",
    "\n",
    "# Dawgz library (used to parallelized the jobs)\n",
    "from dawgz import job, schedule\n",
    "\n",
    "# Combinatorics\n",
    "from itertools import combinations, product\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "#\n",
    "#                                  DAWGZ\n",
    "#\n",
    "# ---------------------------------------------------------------------\n",
    "#\n",
    "# -------------\n",
    "# Possibilities\n",
    "# -------------\n",
    "# Creation of all the inputs combinations\n",
    "input_list = [\"temperature\"]\n",
    "\n",
    "# Generate all combinations\n",
    "all_combinations = []\n",
    "for r in range(1, len(input_list) + 1):\n",
    "    all_combinations.extend(combinations(input_list, r))\n",
    "\n",
    "# Convert combinations to lists\n",
    "all_combinations = [list(combination) for combination in all_combinations]\n",
    "\n",
    "# Storing all the information\n",
    "arguments = {\n",
    "    'month_start'     : [0],\n",
    "    'month_end'       : [1],\n",
    "    'year_start'      : [0],\n",
    "    'year_end'        : [0],\n",
    "    'Inputs'          : all_combinations,\n",
    "    'Problem'         : [\"regression\", \"classification\"],\n",
    "    'Window (Inputs)' : [1],\n",
    "    'Window (Output)' : [1],\n",
    "    'Depth'           : [200],\n",
    "    'Architecture'    : [\"FCNN\"],\n",
    "    'Learning Rate'   : [0.001],\n",
    "    'Kernel Size'     : [3],\n",
    "    'Batch Size'      : [64],\n",
    "    'Epochs'          : [3]\n",
    "}\n",
    "\n",
    "# Generate all combinations\n",
    "param_combinations = list(product(*arguments.values()))\n",
    "\n",
    "# Create a list of dictionaries\n",
    "param_dicts = [dict(zip(arguments.keys(), combo)) for combo in param_combinations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6b1b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(**kwargs):\n",
    "\n",
    "    # ------------------------------------------\n",
    "    #               Initialization\n",
    "    # ------------------------------------------\n",
    "    #\n",
    "    # ------- Arguments -------\n",
    "    start_month     = kwargs['month_start']\n",
    "    end_month       = kwargs['month_end']\n",
    "    start_year      = kwargs['year_start']\n",
    "    end_year        = kwargs['year_end']\n",
    "    inputs          = kwargs['Inputs']\n",
    "    problem         = kwargs['Problem']\n",
    "    windows_inputs  = kwargs['Window (Inputs)']\n",
    "    windows_outputs = kwargs['Window (Output)']\n",
    "    depth           = kwargs['Depth']\n",
    "    architecture    = kwargs['Architecture']\n",
    "    learning_rate   = kwargs['Learning Rate']\n",
    "    kernel_size     = kwargs['Kernel Size']\n",
    "    batch_size      = kwargs['Batch Size']\n",
    "    nb_epochs       = kwargs['Epochs']\n",
    "\n",
    "    # ------- Data -------\n",
    "    Dataset_phy = BlackSea_Dataset(year_start = start_year, year_end = end_year, month_start = start_month,  month_end = end_month, variable = \"grid_T\")\n",
    "    Dataset_bio = BlackSea_Dataset(year_start = start_year, year_end = end_year, month_start = start_month,  month_end = end_month, variable = \"ptrc_T\")\n",
    "\n",
    "    # Loading the inputs\n",
    "    input_datasets = list()\n",
    "    for inp in inputs:\n",
    "        if inp in [\"temperature\", \"salinity\"]:\n",
    "            input_datasets.append(Dataset_phy.get_data(variable = inp, type = \"surface\", depth = None))\n",
    "        if inp in [\"chlorophyll\", \"kshort\", \"klong\"]:\n",
    "            input_datasets.append(Dataset_bio.get_data(variable = inp, type = \"surface\", depth = None))\n",
    "\n",
    "    # Loading the output\n",
    "    data_oxygen = Dataset_bio.get_data(variable = \"oxygen\", type = \"bottom\", depth = depth)\n",
    "\n",
    "    # Loading the black sea mask\n",
    "    bs_mask             = Dataset_phy.get_mask(depth = None)\n",
    "    bs_mask_with_depth  = Dataset_phy.get_mask(depth = depth)\n",
    "\n",
    "    # ------- Preprocessing -------\n",
    "    BSD_loader = BlackSea_Dataloader(x = input_datasets,\n",
    "                                     y = data_oxygen,\n",
    "                               bs_mask = bs_mask,\n",
    "                    bs_mask_with_depth = bs_mask_with_depth,\n",
    "                                  mode = problem,\n",
    "                            window_inp = windows_inputs,\n",
    "                            window_out = windows_outputs,\n",
    "                      hypoxia_treshold = 63,\n",
    "                         datasets_size = [0.6, 0.3],\n",
    "                                  seed = 2701)\n",
    "\n",
    "    # Retreiving the individual dataloader\n",
    "    dataset_train      = BSD_loader.get_dataloader(\"train\",      batch_size = batch_size)\n",
    "    dataset_validation = BSD_loader.get_dataloader(\"validation\", batch_size = batch_size)\n",
    "    dataset_test       = BSD_loader.get_dataloader(\"test\",       batch_size = batch_size)\n",
    "\n",
    "    # Normalized oxygen treshold\n",
    "    norm_oxy = BSD_loader.get_normalized_deoxygenation_treshold()\n",
    "\n",
    "    # ------------------------------------------\n",
    "    #                   Training\n",
    "    # ------------------------------------------\n",
    "    #\n",
    "    # ------- WandB -------\n",
    "    wandb.init(project = \"esa-blacksea-deoxygenation-emulator-V3\", config = kwargs)\n",
    "\n",
    "    # Check if GPU is available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Initialization of neural network and pushing it to device (GPU)\n",
    "    neural_net = FCNN(inputs = len(input_datasets), outputs =  windows_outputs, problem = problem, kernel_size = kernel_size)\n",
    "    neural_net.to(device)\n",
    "\n",
    "    # Initialization of the optimizer and the loss function\n",
    "    optimizer  = optim.Adam(neural_net.parameters(), lr = learning_rate)\n",
    "    criterion  = nn.MSELoss() if problem == \"regression\" else nn.BCELoss()\n",
    "\n",
    "    # Used to compute time left\n",
    "    epoch_time = 0.0\n",
    "\n",
    "    # Starting training !\n",
    "    for epoch in range(nb_epochs):\n",
    "\n",
    "        # Information over terminal (1)\n",
    "        print(\"\\n\") if epoch == 0 else print(\"\")\n",
    "        print(\"Epoch : \", epoch + 1, \"/\", nb_epochs, \"\\n\")\n",
    "\n",
    "        # Used to approximate time left for current epoch and in total\n",
    "        start      = time.time()\n",
    "\n",
    "        # Used to store instantaneous loss and compute the average per batch (AOB) training loss\n",
    "        training_loss = 0.0\n",
    "        batch_steps   = 0\n",
    "\n",
    "        # Used to compute our metrics\n",
    "        metrics_tool = BlackSea_Metrics(mode = problem,\n",
    "                                        mask = bs_mask_with_depth,\n",
    "                                        treshold = norm_oxy,\n",
    "                                        number_of_batches = len(dataset_validation))\n",
    "\n",
    "        # ----- TRAINING -----\n",
    "        for x, y in dataset_train:\n",
    "\n",
    "            # Moving data to the correct device\n",
    "            x, y = to_device(x, device), to_device(y, device)\n",
    "\n",
    "            # Forward pass, i.e. prediction of the neural network\n",
    "            pred = neural_net.forward(x)\n",
    "\n",
    "            # Determine the indices of the valid samples, i.e. inside the observed region (-1 is the masked region)\n",
    "            indices = torch.where(y != -1)\n",
    "\n",
    "            # Computing the loss\n",
    "            loss = criterion(pred[indices], y[indices])\n",
    "\n",
    "            # Information over terminal (2)\n",
    "            print(\"Loss (T) = \", loss.detach().item())\n",
    "\n",
    "            # Sending to wandDB\n",
    "            wandb.log({\"Loss (T)\": loss.detach().item()})\n",
    "\n",
    "            # Accumulating the loss\n",
    "            training_loss += loss.detach().item()\n",
    "\n",
    "            # Reseting the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Optimizing the parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            # Updating epoch information\n",
    "            batch_steps += 1\n",
    "\n",
    "            break\n",
    "\n",
    "        # Information over terminal (3)\n",
    "        print(\"Loss (Training, Averaged over batch): \", training_loss / batch_steps)\n",
    "\n",
    "        # Sending the loss to wandDB\n",
    "        wandb.log({\"Loss (T, AOB): \": training_loss / batch_steps})\n",
    "\n",
    "        # ----- VALIDATION -----\n",
    "        with torch.no_grad():\n",
    "\n",
    "            # Used to store instantaneous loss and compute the average per batch (AOB) training loss\n",
    "            validation_loss = 0.0\n",
    "            batch_steps = 0\n",
    "\n",
    "            for x, y in dataset_validation:\n",
    "\n",
    "                # Moving data to the correct device\n",
    "                x, y = to_device(x, device), to_device(y, device)\n",
    "\n",
    "                # Forward pass, i.e. prediction of the neural network\n",
    "                pred = neural_net.forward(x)\n",
    "\n",
    "                # Determine the indices of the valid samples, i.e. inside the observed region (-1 is the masked region)\n",
    "                indices = torch.where(y != -1)\n",
    "\n",
    "                # Computing the loss\n",
    "                loss = criterion(pred[indices], y[indices])\n",
    "\n",
    "                # Information over terminal (4)\n",
    "                print(\"Loss (V) = \", loss.detach().item())\n",
    "\n",
    "                # Sending the loss to wandDB the loss\n",
    "                wandb.log({\"Loss (V)\": loss.detach().item()})\n",
    "\n",
    "                # Accumulating the loss\n",
    "                validation_loss += loss.detach().item()\n",
    "\n",
    "                # Used to compute the metrics\n",
    "                metrics_tool.compute_metrics(y_pred = pred.cpu(), y_true = y.cpu())\n",
    "\n",
    "                # Visual inspection (Only on the first batch)\n",
    "                metrics_tool.compute_plots(y_pred = pred.cpu(), y_true = y.cpu()) if batch_steps == 0 else None\n",
    "\n",
    "                # Updating epoch information\n",
    "                batch_steps += 1\n",
    "\n",
    "                break\n",
    "\n",
    "            # Information over terminal (5)\n",
    "            print(\"Loss (Validation, Averaged over batch): \", validation_loss / batch_steps)\n",
    "\n",
    "            # Sending more information to wandDB\n",
    "            wandb.log({\"Loss (V, AOB): \": validation_loss / batch_steps})\n",
    "            wandb.log({\"Epochs : \": nb_epochs - epoch})\n",
    "\n",
    "            # ---------- WandB (Metrics & Plots) ----------\n",
    "            #\n",
    "            # Getting results of each metric (averaged over each batch)\n",
    "            results = metrics_tool.get_results()\n",
    "            results_name = metrics_tool.get_names_metrics()\n",
    "\n",
    "            # Sending these results to wandDB\n",
    "            for d, day_results in enumerate(results):\n",
    "                for i, result in enumerate(day_results):\n",
    "\n",
    "                    # Current name of metric with corresponding day\n",
    "                    m_name = results_name[i] + \" D(\" + str(d) + \")\"\n",
    "\n",
    "                    # Logging\n",
    "                    wandb.log({m_name : result})\n",
    "\n",
    "            # Getting the plots\n",
    "            plots = metrics_tool.get_plots()\n",
    "\n",
    "            # Sending the plots to wandDB\n",
    "            for p_info in plots:\n",
    "\n",
    "                    # Ease of comprehension\n",
    "                    p_fig = p_info[0]\n",
    "                    p_nam = p_info[1]\n",
    "\n",
    "                    # Logging\n",
    "                    wandb.log({p_nam : wandb.Image(p_fig)})\n",
    "\n",
    "        # Updating timing\n",
    "        epoch_time = time.time() - start\n",
    "\n",
    "    # Finishing the run\n",
    "    wandb.finish()\n",
    "\n",
    "##############################################################################################################\n",
    "main(**param_dicts[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92aba726",
   "metadata": {},
   "source": [
    "<hr style=\"color:#5A7D9F;\">\n",
    "<p align=\"center\">\n",
    "    <b style=\"font-size:2vw; color:#5A7D9F; font-weight:bold;\">\n",
    "    <center>Testing</center>\n",
    "    </b>\n",
    "</p>\n",
    "<hr style=\"color:#5A7D9F;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fc96d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#       TESTING\n",
    "# --------------------\n",
    "#\n",
    "# Number of input variables\n",
    "var_inputs = 2\n",
    "\n",
    "# Window for the input\n",
    "win_in = 2\n",
    "\n",
    "# Window for the oxygen\n",
    "win_out = 3\n",
    "\n",
    "# Number of days\n",
    "days = 14\n",
    "\n",
    "# --------------------\n",
    "#      GENERATING\n",
    "# --------------------\n",
    "# Generating fake data\n",
    "fake_data_physical_variables = generateFakeDataset(resolution = 128, number_of_variables = var_inputs, number_of_samples=days)\n",
    "fake_data_physical_oxygen    = generateFakeDataset(resolution = 128, number_of_variables = 1, number_of_samples=days, oxygen = True)\n",
    "\n",
    "# Creation of the dataloaders\n",
    "BSD_loader_fake_spatial = BlackSea_Dataloader(x = fake_data_physical_variables,\n",
    "                                              y = fake_data_physical_oxygen[0],\n",
    "                                           mask = np.ones(shape = (258, 258)),\n",
    "                                           mode = \"spatial\",\n",
    "                                     resolution = 128,\n",
    "                                        window = win_in,\n",
    "                                    window_out = win_out)\n",
    "\n",
    "BSD_loader_fake_temporal = BlackSea_Dataloader(x = fake_data_physical_variables,\n",
    "                                               y = fake_data_physical_oxygen[0],\n",
    "                                            mask = np.ones(shape = (258, 258)),\n",
    "                                            mode = \"temporal\",\n",
    "                                      resolution = 128,\n",
    "                                          window = win_in,\n",
    "                                      window_out = win_out)\n",
    "\n",
    "\n",
    "# OK\n",
    "\"\"\"\n",
    "for i in range(fake_data_physical_variables[0].shape[0]):\n",
    "\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(fake_data_physical_variables[1][i, :, :])\n",
    "\"\"\"\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "#                                       SPATIAL\n",
    "# --------------------------------------------------------------------------------\n",
    "\"\"\"\n",
    "for x, y in BSD_loader_fake_spatial.get_dataloader(\"train\"):\n",
    "\n",
    "    # Initial shapes\n",
    "    print(\"Input shape: \", len(fake_data_physical_variables), fake_data_physical_variables[0].shape, \"\\nOutput shape: \", fake_data_physical_oxygen[0].shape)\n",
    "\n",
    "    # Shapes (dataloader)\n",
    "    print(\"Input shape: \", x.shape, \"\\nOutput shape: \", y.shape)\n",
    "\n",
    "    # The total number of samples is\n",
    "    # [Number of timesteps - number of input days (window) - number of output days (window_outgen) ] * number of regions\n",
    "    #\n",
    "    # Tests\n",
    "    #\n",
    "    # Number of variables\n",
    "    assert x.shape[1] == var_inputs * win_in\n",
    "\n",
    "    # Number of outputs\n",
    "    assert y.shape[1] == win_out\n",
    "\n",
    "    # Number of samples (must be divided by 2 for the validation and test)\n",
    "    assert x.shape[0] == (fake_data_physical_variables[0].shape[0] - win_in - win_out) * int(256/128)\n",
    "\n",
    "    # Checking that I have all the timesteps\n",
    "    #\n",
    "    # Looping over all the time steps\n",
    "    for i in range(x.shape[0]):\n",
    "\n",
    "        # Showing as a subplots the input and output pairs\n",
    "        plt.figure(figsize=(14, 14))\n",
    "\n",
    "        for j in range(var_inputs * win_in):\n",
    "            plt.subplot(1, var_inputs * win_in + win_out, j+1)\n",
    "\n",
    "            # Removing labels and tickz\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "            plt.grid(False)\n",
    "            plt.imshow(x[i, j, :, :])\n",
    "\n",
    "        for j in range(win_out):\n",
    "            plt.subplot(1, var_inputs * win_in + win_out, var_inputs * win_in + j+1)\n",
    "            # Removing labels and tickz\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "            plt.grid(False)\n",
    "\n",
    "            plt.imshow(y[i, j, :, :])\n",
    "\n",
    "    # Idea check the validity using a trehsold to create a 1 0 matrix for comparison (otherwise it will  bug since you have normalized the data)\n",
    "\n",
    "\"\"\"\n",
    "# --------------------------------------------------------------------------------\n",
    "#                                       SPATIAL\n",
    "# --------------------------------------------------------------------------------\n",
    "for x, y in BSD_loader_fake_temporal.get_dataloader(\"train\"):\n",
    "\n",
    "    # Initial shapes\n",
    "    print(\"Input shape: \", len(fake_data_physical_variables), fake_data_physical_variables[0].shape, \"\\nOutput shape: \", fake_data_physical_oxygen[0].shape)\n",
    "\n",
    "    # Shapes (dataloader)\n",
    "    print(\"Input shape: \", x.shape, \"\\nOutput shape: \", y.shape)\n",
    "\n",
    "    # Checking that I have all the regions\n",
    "    #\n",
    "    # Looping over all the time steps\n",
    "    for i in range(x.shape[0]):\n",
    "\n",
    "        # Showing as a subplots the input and output pairs\n",
    "        plt.figure(figsize=(14, 14))\n",
    "\n",
    "        for j in range(var_inputs * win_in):\n",
    "            plt.subplot(1, var_inputs * win_in + win_out, j+1)\n",
    "\n",
    "            # Removing labels and tickz\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "            plt.grid(False)\n",
    "            plt.imshow(x[i, j, :, :])\n",
    "\n",
    "        for j in range(win_out):\n",
    "            plt.subplot(1, var_inputs * win_in + win_out, var_inputs * win_in + j+1)\n",
    "            # Removing labels and tickz\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "            plt.grid(False)\n",
    "\n",
    "            plt.imshow(y[i, j, :, :])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
