{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a999b204-3b95-404f-93a1-90b1bda33abb",
   "metadata": {},
   "source": [
    "<img src=\"../assets/header_notebook.png\" />\n",
    "<hr style=\"color:#5A7D9F;\">\n",
    "<p align=\"center\">\n",
    "    <b style=\"font-size:2vw; color:#5A7D9F; font-weight:bold;\">\n",
    "    <center>ESA - Black Sea Deoxygenation Emulator</center>\n",
    "    </b>\n",
    "</p>\n",
    "<hr style=\"color:#5A7D9F;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b55dcb8-9067-463c-b876-2e45565d6240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------\n",
    "# Librairies\n",
    "# ----------\n",
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import dawgz\n",
    "import wandb\n",
    "import xarray\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Dawgz (jobs //)\n",
    "from dawgz import job, schedule\n",
    "\n",
    "# -------------------\n",
    "# Librairies (Custom)\n",
    "# -------------------\n",
    "# Adding path to source folder to load custom modules\n",
    "sys.path.append('/src')\n",
    "sys.path.append('/src/debs/')\n",
    "sys.path.insert(1, '/src/debs/')\n",
    "sys.path.insert(1, '/scripts/')\n",
    "\n",
    "# Moving to the .py directory\n",
    "%cd src/debs/\n",
    "\n",
    "## Loading libraries\n",
    "from metrics     import *\n",
    "from dataset     import *\n",
    "from dataloader  import *\n",
    "from tools       import *\n",
    "from losses      import *\n",
    "\n",
    "# -------\n",
    "# Jupyter\n",
    "# -------\n",
    "%matplotlib inline\n",
    "plt.rcParams.update({'font.size': 13})\n",
    "\n",
    "# Making sure modules are reloaded when modified\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9c007b-1cab-4407-a5f0-cbcdd836a569",
   "metadata": {},
   "source": [
    "<hr style=\"color:#5A7D9F;\">\n",
    "<p align=\"center\">\n",
    "    <b style=\"font-size:2vw; color:#5A7D9F; font-weight:bold;\">\n",
    "    <center>SCRIPTS</center>\n",
    "    </b>\n",
    "</p>\n",
    "<hr style=\"color:#5A7D9F;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfe6b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating the preprocess data\n",
    "%run generate.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23f012a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training a neural network:\n",
    "%run __training.py --config local"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9aa051",
   "metadata": {},
   "source": [
    "<hr style=\"color:#5A7D9F;\">\n",
    "<p align=\"center\">\n",
    "    <b style=\"font-size:2vw; color:#5A7D9F; font-weight:bold;\">\n",
    "    <center>Playground</center>\n",
    "    </b>\n",
    "</p>\n",
    "<hr style=\"color:#5A7D9F;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6356ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------\n",
    "#    Parameters\n",
    "# -----------------\n",
    "#\n",
    "# Time window\n",
    "month_starting = 1\n",
    "month_ending   = 3\n",
    "year_starting  = 1980\n",
    "year_ending    = 1980\n",
    "\n",
    "# ------------------\n",
    "#  Loading the data\n",
    "# ------------------\n",
    "# Loading the different datasets\n",
    "BSD_dataset = BlackSea_Dataset(year_start  = year_starting,\n",
    "                               year_end    = year_ending,\n",
    "                               month_start = month_starting,\n",
    "                               month_end   = month_ending)\n",
    "\n",
    "# Loading the days ID (used to give temporal information to the model)\n",
    "days_ID = BSD_dataset.get_days()\n",
    "\n",
    "# Loading the different inputs\n",
    "data_temperature   = BSD_dataset.get_data(variable = \"temperature\")\n",
    "data_salinity      = BSD_dataset.get_data(variable = \"salinity\")\n",
    "data_chlorophyll   = BSD_dataset.get_data(variable = \"chlorophyll\")\n",
    "data_kshort        = BSD_dataset.get_data(variable = \"kshort\")\n",
    "data_klong         = BSD_dataset.get_data(variable = \"klong\")\n",
    "\n",
    "# Loading the output\n",
    "data_oxygen = BSD_dataset.get_data(variable = \"oxygen\")\n",
    "\n",
    "# Loading spatial information\n",
    "bathy = BSD_dataset.get_depth(unit = \"meter\")\n",
    "mesh  = BSD_dataset.get_mesh(x = 256, y = 576)\n",
    "\n",
    "# Loading the black sea masks\n",
    "bs_mask             = BSD_dataset.get_mask(continental_shelf = False)\n",
    "bs_mask_with_depth  = BSD_dataset.get_mask(continental_shelf = True)\n",
    "bs_mask_complete    = get_complete_mask(data_oxygen, bs_mask_with_depth)\n",
    "\n",
    "# Hypoxia treshold\n",
    "hypox_tresh = xarray.open_dataset(BSD_dataset.paths[0])[\"HYPON\"].data.item()\n",
    "\n",
    "# -----------------------\n",
    "#  Preprocessing the data\n",
    "# -----------------------\n",
    "#\n",
    "# Creation of the dataloader\n",
    "BSD_loader = BlackSea_Dataloader(x = [data_temperature],\n",
    "                                 y = data_oxygen,\n",
    "                                 t = days_ID,\n",
    "                              mesh = mesh,\n",
    "                              mask = bs_mask,\n",
    "                   mask_with_depth = bs_mask_with_depth,\n",
    "                        bathymetry = bathy,\n",
    "                        window_inp = 1,\n",
    "                        window_out = 1,\n",
    "                    window_transfo = 1,\n",
    "                              mode = \"regression\",\n",
    "                  hypoxia_treshold = hypox_tresh)\n",
    "\n",
    "# Retrieving the datasets\n",
    "ds_train      = BSD_loader.get_dataloader(\"train\")\n",
    "ds_validation = BSD_loader.get_dataloader(\"validation\")\n",
    "ds_test       = BSD_loader.get_dataloader(\"test\")\n",
    "\n",
    "# Extracting the normalized oxygen treshold value\n",
    "norm_oxy = BSD_loader.get_normalized_deoxygenation_treshold()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c985c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, t, y in ds_train:\n",
    "\n",
    "    print(x.shape)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95e2d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------\n",
    "#\n",
    "#        |\n",
    "#       / \\\n",
    "#      / _ \\                  ESA - PROJECT\n",
    "#     |.o '.|\n",
    "#     |'._.'|          BLACK SEA DEOXYGENATION EMULATOR\n",
    "#     |     |\n",
    "#   ,'|  |  |`.             BY VICTOR MANGELEER\n",
    "#  /  |  |  |  \\\n",
    "#  |,-'--|--'-.|                2023-2024\n",
    "#\n",
    "#\n",
    "# -------------------------------------------------------\n",
    "#\n",
    "# Documentation\n",
    "# -------------\n",
    "# A neural network definition to be used as temporal encoder\n",
    "#\n",
    "# Pytorch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class ENCODER(nn.Sequential):\n",
    "    r\"\"\"A neural network used to encode the temporal information of the data and return weights for the input data\"\"\"\n",
    "\n",
    "    def __init__(self, input_size : int):\n",
    "        super(ENCODER, self).__init__()\n",
    "\n",
    "        # Defining the layers\n",
    "        self.linear_in       = nn.Linear(input_size, 256)\n",
    "        self.linear_middle_1 = nn.Linear(256,        256)\n",
    "        self.linear_middle_2 = nn.Linear(256,        128)\n",
    "        self.linear_middle_3 = nn.Linear(128,         64)\n",
    "        self.linear_middle_4 = nn.Linear(64,          32)\n",
    "        self.linear_out      = nn.Linear(32,           1)\n",
    "\n",
    "        # Defining the activation functions\n",
    "        self.activation = nn.GELU()\n",
    "\n",
    "        # Defining the softmax function, i.e. (t, values, day) to (t, values, 1) then (t, weights, 1)\n",
    "        self.softmax = nn.Softmax(dim = 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Applying the layers\n",
    "        x = self.activation(self.linear_in(x))\n",
    "        x = self.activation(self.linear_middle_1(x))\n",
    "        x = self.activation(self.linear_middle_2(x))\n",
    "        x = self.activation(self.linear_middle_3(x))\n",
    "        x = self.activation(self.linear_middle_4(x))\n",
    "        x = self.linear_out(x)\n",
    "\n",
    "        # Applying the softmax function\n",
    "        return self.softmax(x)\n",
    "\n",
    "    def count_parameters(self,):\n",
    "        r\"\"\"Determines the number of trainable parameters in the model\"\"\"\n",
    "        return int(sum(p.numel() for p in self.parameters() if p.requires_grad))\n",
    "\n",
    "class FCNN(nn.Sequential):\n",
    "    r\"\"\"A fully convolutional neural network\"\"\"\n",
    "\n",
    "    def __init__(self, problem: str, inputs: int, outputs: int, window_transformation: int = 1, kernel_size : int = 3, scaling : int = 1):\n",
    "        super(FCNN, self).__init__()\n",
    "\n",
    "        # Initialization\n",
    "        self.n_in    = inputs\n",
    "        self.problem = problem\n",
    "        self.padding = kernel_size // 2\n",
    "\n",
    "        # Number of output channels, i.e. times 2 because either mean and std for regression or both classes for classification\n",
    "        self.n_out   = outputs * 2\n",
    "\n",
    "        # ------ Architecture ------\n",
    "        #\n",
    "        # Temporal Encoder\n",
    "        self.block_encoder = ENCODER(window_transformation)\n",
    "\n",
    "        # Main Layers\n",
    "        self.conv_init           = nn.Conv2d(self.n_in    , 256 * scaling, kernel_size, padding = self.padding)\n",
    "        self.conv_intermediate_1 = nn.Conv2d(256 * scaling, 128 * scaling, kernel_size, padding = self.padding)\n",
    "        self.conv_intermediate_2 = nn.Conv2d(128 * scaling,  64 * scaling, kernel_size, padding = self.padding)\n",
    "        self.conv_intermediate_3 = nn.Conv2d( 64 * scaling,  32 * scaling, kernel_size, padding = self.padding)\n",
    "        self.conv_final          = nn.Conv2d( 32 * scaling,    self.n_out, kernel_size, padding = self.padding)\n",
    "\n",
    "        # Activation function\n",
    "        self.activation = nn.GELU()\n",
    "\n",
    "        # Normalization\n",
    "        self.normalization_init           = nn.BatchNorm2d(self.conv_init.out_channels)\n",
    "        self.normalization_intermediate_1 = nn.BatchNorm2d(self.conv_intermediate_1.out_channels)\n",
    "        self.normalization_intermediate_2 = nn.BatchNorm2d(self.conv_intermediate_2.out_channels)\n",
    "        self.normalization_intermediate_3 = nn.BatchNorm2d(self.conv_intermediate_3.out_channels)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "\n",
    "        # Retrieiving dimensions (Ease of comprehension)\n",
    "        samples, days, values, variables, x_res, y_res = x.shape\n",
    "\n",
    "        # ----- Encoding Time -----\n",
    "        #\n",
    "        # Applying the encoder\n",
    "        weights = torch.squeeze(self.block_encoder(t), dim = -1)\n",
    "\n",
    "        # Applying the weights (except to mesh (dim = 2) and bathymetry (dim = 3))\n",
    "        for sample in range(samples):\n",
    "            for value in range(days):\n",
    "                x[:, value, :, :-3] *= weights[sample, value]\n",
    "\n",
    "        # Reshaping\n",
    "        x = x.reshape(samples, days * values * variables, x_res, y_res)\n",
    "\n",
    "        # ----- Fully Convolutionnal -----\n",
    "        #\n",
    "        x = self.normalization_init(self.activation(self.conv_init(x)))\n",
    "        x = self.normalization_intermediate_1(self.activation(self.conv_intermediate_1(x)))\n",
    "        x = self.normalization_intermediate_2(self.activation(self.conv_intermediate_2(x)))\n",
    "        x = self.normalization_intermediate_3(self.activation(self.conv_intermediate_3(x)))\n",
    "        x = self.conv_final(x)\n",
    "\n",
    "        # Retrieiving dimensions (Ease of comprehension)\n",
    "        b, c, x_res, y_res = x.shape\n",
    "\n",
    "        # Reshaping the output, i.e. (samples, days, values, x, y)\n",
    "        return x.reshape(b, self.n_out // 2, 2, x_res, y_res)\n",
    "\n",
    "    def count_parameters(self,):\n",
    "        r\"\"\"Determines the number of trainable parameters in the model\"\"\"\n",
    "        return int(sum(p.numel() for p in self.parameters() if p.requires_grad))\n",
    "\n",
    "class AVERAGE(nn.Sequential):\n",
    "    r\"\"\"A 'neural network' that predicts the pixel temporal average (should be used a baseline)\"\"\"\n",
    "\n",
    "    def __init__(self, data_output : np.array, device : str, kwargs : dict):\n",
    "        super(AVERAGE, self).__init__()\n",
    "\n",
    "        # Extracting information\n",
    "        dataset_size     = [0.6, 0.3]\n",
    "        problem          = \"regression\"\n",
    "        hypoxia_treshold = 0.1\n",
    "\n",
    "        # Retrieiving dimensions\n",
    "        t, x, y = data_output.shape\n",
    "\n",
    "        # Number of training samples\n",
    "        train_samples = int(t * dataset_size[0])\n",
    "\n",
    "        # ----- Regression ------\n",
    "        if problem == \"regression\":\n",
    "\n",
    "            # Determine the minimum and maximum values of the data\n",
    "            min_value = np.nanmin(data_output)\n",
    "            max_value = np.nanmax(data_output)\n",
    "\n",
    "            # Determining the minimum and maximum values\n",
    "            min_value = np.nanmin(data_output)\n",
    "            max_value = np.nanmax(data_output)\n",
    "\n",
    "            # Shift the data to ensure minimum value is 0\n",
    "            shifted_data = data_output - min_value\n",
    "\n",
    "            # Normalizing the data\n",
    "            normalized_data = shifted_data / (max_value - min_value)\n",
    "\n",
    "            # Predicting the average and log of variance\n",
    "            average_output = torch.mean(torch.from_numpy(normalized_data[: train_samples, :, :]), dim = 0)\n",
    "            std_output     = torch.log(torch.var(torch.from_numpy(normalized_data[: train_samples, :, :]), dim = 0))\n",
    "\n",
    "            # Stacking\n",
    "            average_output = torch.stack([average_output, std_output])\n",
    "\n",
    "        # ----- Classification ------\n",
    "        else:\n",
    "\n",
    "            # Converting to classification\n",
    "            average_output = torch.from_numpy((data_output[: train_samples, :, :] < hypoxia_treshold) * 1)\n",
    "\n",
    "            # Summing over time, i.e. if total number of hypoxic days is greater than 50% of the time, then it is hypoxic\n",
    "            average_output = (torch.sum(average_output, dim = 0) > train_samples // 2) * 1\n",
    "\n",
    "            # Conversion to \"probabilities\", i.e. (t, x, y) to (t, c, x, y) with c = 0 no hypoxia, c = 1 hypoxia\n",
    "            average_output = torch.stack([(average_output == 0) * 1, average_output]).float()\n",
    "\n",
    "        # Storing information\n",
    "        self.outputs = 1\n",
    "        self.bs      = 64\n",
    "        self.average = self.process(average_output)\n",
    "        self.device  = \"cuda\"\n",
    "\n",
    "        # Dummy feature (It plays no role whatsoever, it is just a placeholder to make the model work with the trainer)\n",
    "        self.layer = nn.Conv2d(1, 1, 1)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        return to_device(self.average[:x.shape[0]], self.device)\n",
    "\n",
    "    def process(self, x : torch.Tensor):\n",
    "        r\"\"\"Used to format the output to the correct shape\"\"\"\n",
    "\n",
    "        # Adding number of forecasted days\n",
    "        x = torch.unsqueeze(x, dim = 0) if self.outputs == 1 else \\\n",
    "            torch.stack([x for i in range(self.outputs)], dim = 0)\n",
    "\n",
    "        # Adding batch size\n",
    "        return torch.stack([x for i in range(self.bs)], dim = 0)\n",
    "\n",
    "    def count_parameters(self,):\n",
    "        r\"\"\"Determines the number of trainable parameters in the model\"\"\"\n",
    "        return int(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76799521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network stuff\n",
    "neural_network = FCNN(problem = \"regression\",\n",
    "                      inputs  = 4,\n",
    "                      outputs = 1,\n",
    "                      window_transformation = 1,\n",
    "                      kernel_size = 3,\n",
    "                      scaling = 1)\n",
    "\n",
    "\n",
    "#neural_network = AVERAGE(data_oxygen, \"cuda\", {})\n",
    "\n",
    "\n",
    "optimizer      = optim.Adam(neural_network.parameters(), lr = 0.001)\n",
    "\n",
    "# Check if a GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "neural_network.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa9a122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "show = True\n",
    "\n",
    "for epoch in range(1):\n",
    "\n",
    "    metrics_tool = BlackSea_Metrics(mode = \"regression\",\n",
    "                                    mask = bs_mask_with_depth,\n",
    "                           mask_complete = bs_mask_complete,\n",
    "                                treshold = norm_oxy,\n",
    "                       number_of_samples = BSD_loader.get_number_of_samples(\"validation\"))\n",
    "\n",
    "    for x, t, y in ds_train:\n",
    "\n",
    "        x, t, y = x.to(device), t.to(device), y.to(device)\n",
    "        prediction = neural_network(x, t)\n",
    "        loss_training = compute_loss(y_pred = prediction, y_true = y,mask = bs_mask_with_depth, problem = \"regression\", device = \"cpu\", kwargs = {})\n",
    "        print(f\"E{epoch} - Loss (Training):\", loss_training.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss_training.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Cleaning\n",
    "        del x, t, y, prediction, loss_training\n",
    "        torch.cuda.empty_cache()\n",
    "        break\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # Stores all the predictions for the metrics (plots)\n",
    "        prediction_all = None\n",
    "\n",
    "        for x, t, y in ds_validation:\n",
    "\n",
    "            # Making prediction\n",
    "            x, t, y = x.to(device), t.to(device), y.to(device)\n",
    "            prediction = neural_network(x, t)\n",
    "            loss_validation = compute_loss(y_pred = prediction, y_true = y, mask = bs_mask_with_depth, problem = \"regression\", device = \"cpu\", kwargs = {})\n",
    "            print(f\"E{epoch} - Loss (Validation):\", loss_validation.item())\n",
    "            x, t, y, prediction = x.to(\"cpu\"), t.to(\"cpu\"), y.to(\"cpu\"), prediction.to(\"cpu\")\n",
    "\n",
    "            \"\"\"\n",
    "            # Plotting mean against ground truth in a subplot\n",
    "            if show:\n",
    "\n",
    "                # Highlighting hypoxic areas\n",
    "                y_hyp = ( y < norm_oxy ) * 1.0\n",
    "                p_hyp = ( prediction < norm_oxy ) * 1.0\n",
    "\n",
    "                # Hiding non-obserable areas\n",
    "                p_hyp[:,:,:, y[0, 0, 0] == -1] = torch.nan\n",
    "                y_hyp[:,:,:, y[0, 0, 0] == -1] = torch.nan\n",
    "\n",
    "\n",
    "                plt.figure(figsize = (20, 20))\n",
    "                plt.subplot(1, 3, 1)\n",
    "                plt.imshow(torch.flipud(y_hyp[0, 0, 0]))\n",
    "                plt.subplot(1, 3, 2)\n",
    "                plt.imshow(torch.flipud(p_hyp[0, 0, 0]))\n",
    "                plt.subplot(1, 3, 3)\n",
    "                plt.imshow(torch.flipud(y_hyp[0, 0, 0]) - torch.flipud(p_hyp[0, 0, 0]))\n",
    "                plt.setp(plt.gcf().get_axes(), xticks = [], yticks = [])\n",
    "                plt.subplot(1, 3, 1).set_title(\"Ground Truth\", fontsize = 6)\n",
    "                plt.subplot(1, 3, 2).set_title(\"Prediction\", fontsize = 6)\n",
    "                plt.subplot(1, 3, 3).set_title(\"Difference\", fontsize = 6)\n",
    "                plt.show()\n",
    "\n",
    "                prediction[:,:,:, y[0,0,0,:,:] == -1] = torch.nan\n",
    "                y[:,:,:, y[0,0,0,:,:] == -1] = torch.nan\n",
    "\n",
    "                plt.figure(figsize = (20, 20))\n",
    "                plt.subplot(1, 3, 1)\n",
    "                plt.imshow(torch.flipud(y[0, 0, 0]),  vmin = 0, vmax = 1)\n",
    "                plt.subplot(1, 3, 2)\n",
    "                plt.imshow(torch.flipud(prediction[0, 0, 0]), vmin = 0, vmax = 1)\n",
    "                plt.subplot(1, 3, 3)\n",
    "                plt.imshow(torch.flipud(torch.exp(prediction[0, 0, 1]/2)), vmin = 0, vmax = 1)\n",
    "                plt.setp(plt.gcf().get_axes(), xticks = [], yticks = [])\n",
    "                plt.subplot(1, 3, 1).set_title(\"Ground Truth\", fontsize = 6)\n",
    "                plt.subplot(1, 3, 2).set_title(\"Prediction (Mean)\", fontsize = 6)\n",
    "                plt.subplot(1, 3, 3).set_title(\"Prediction (Std)\", fontsize = 6)\n",
    "                plt.show()\n",
    "\n",
    "            \"\"\"\n",
    "\n",
    "            # Concatenating all the predictions\n",
    "            prediction_all = torch.cat((prediction_all, prediction), dim = 0) if prediction_all is not None else prediction\n",
    "\n",
    "            metrics_tool.compute_plots(  y_pred = prediction_all, y_true = y_vall_all)\n",
    "\n",
    "            del x, t, y, prediction\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "\n",
    "    # Metrics\n",
    "    y_vall_all = torch.from_numpy(BSD_loader.y_validation)\n",
    "    metrics_tool.compute_metrics(y_pred = prediction_all, y_true = y_vall_all)\n",
    "    #metrics_tool.compute_plots_comparison_regression(y_pred = prediction_all, y_true = y_vall_all)\n",
    "    metrics_tool.compute_plots(  y_pred = prediction_all, y_true = y_vall_all)\n",
    "\n",
    "    # Getting the results\n",
    "    if show:\n",
    "        results, results_name = metrics_tool.get_results()\n",
    "        for r, n in zip(results[0], results_name):\n",
    "            print(n, \" : \", r)\n",
    "        print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e4ea30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esa",
   "language": "python",
   "name": "esa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
