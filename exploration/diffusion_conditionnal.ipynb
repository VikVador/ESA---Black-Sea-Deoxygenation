{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../assets/header_notebook.png\" />\n",
    "<hr style=\"color:#5A7D9F;\">\n",
    "<p align=\"center\">\n",
    "    <b style=\"font-size:2vw; color:#5A7D9F; font-weight:bold;\">\n",
    "    <center>Diffusion Conditional</center>\n",
    "    </b>\n",
    "</p>\n",
    "<hr style=\"color:#5A7D9F;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch;\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import torch.utils\n",
    "import torch.distributions\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt; plt.rcParams['figure.dpi'] = 200\n",
    "\n",
    "from tqdm import tqdm\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"color:#5A7D9F;\">\n",
    "<p align=\"center\">\n",
    "    <b style=\"font-size:2vw; color:#5A7D9F; font-weight:bold;\">\n",
    "    <center>Loading Data</center>\n",
    "    </b>\n",
    "</p>\n",
    "<hr style=\"color:#5A7D9F;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Composed transforms\n",
    "trs = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),  torchvision.transforms.Pad(2)])\n",
    "\n",
    "# Load the data\n",
    "data_train = torchvision.datasets.MNIST('./data', transform=trs, download=True, train=True)\n",
    "\n",
    "# Create the data loader\n",
    "train_loader = torch.utils.data.DataLoader(data_train, batch_size = 128, shuffle= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying a 5 by 5 plot of image samples\n",
    "data, cls = next(iter(train_loader))\n",
    "plt.figure(figsize = (4, 4))\n",
    "for i in range(50):\n",
    "    plt.subplot(5, 10, i + 1)\n",
    "    plt.imshow(data[i].squeeze(), cmap='gray')\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"color:#5A7D9F;\">\n",
    "<p align=\"center\">\n",
    "<b style=\"font-size:2vw; color:#5A7D9F; font-weight:bold;\">\n",
    "<center>Architecture - Diffusion UNET</center>\n",
    "</b>\n",
    "</p>\n",
    "<hr style=\"color:#5A7D9F;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- #\n",
    "def time_encoding(time: torch.Tensor, frequencies:int = 128):\n",
    "    r\"\"\"Encoding the time using the \"Attention is all you need\" paper encoding scheme\"\"\"\n",
    "\n",
    "    # Security\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # Encoding functions\n",
    "        sinusoidal   = lambda time, frequency_index, frequencies: torch.sin(time / (10000 ** (frequency_index / frequencies)))\n",
    "        cosinusoidal = lambda time, frequency_index, frequencies: torch.cos(time / (10000 ** (frequency_index / frequencies)))\n",
    "\n",
    "        # Storing the encoding\n",
    "        encoded_time = torch.zeros(time.shape[0], time.shape[1], frequencies * 2)\n",
    "\n",
    "        # Mapping time to its encoding\n",
    "        for b_index, b in enumerate(time):\n",
    "            for t_index, t in enumerate(b):\n",
    "\n",
    "                # Stores the current encoding\n",
    "                encoding = list()\n",
    "\n",
    "                # Computing the encoding, i.e. alternating between sinusoidal and cosinusoidal encoding\n",
    "                for i in range(frequencies):\n",
    "                    encoding += [sinusoidal(t, i, frequencies), cosinusoidal(t, i, frequencies)]\n",
    "\n",
    "                # Conversion to torch tensor and storing the encoding\n",
    "                encoded_time[b_index, t_index, :] =  torch.FloatTensor(encoding).clone()\n",
    "\n",
    "        return encoded_time\n",
    "\n",
    "class LayerNormalization(nn.Module):\n",
    "    r\"\"\"Custom Layer Normalization module\"\"\"\n",
    "\n",
    "    def __init__(self, dim: int, eps: float = 1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        var, mean = torch.var_mean(x, dim = self.dim, keepdim = True)\n",
    "        return (x - mean)/torch.sqrt(var + self.eps)\n",
    "\n",
    "class TimeResidual_Block(nn.Module):\n",
    "    r\"\"\"A time residual block for UNET\"\"\"\n",
    "\n",
    "    def __init__(self, input_channels: int, frequencies: int):\n",
    "        super(TimeResidual_Block, self).__init__()\n",
    "\n",
    "        # Initializations\n",
    "        self.frequencies   = frequencies\n",
    "        self.activation    = nn.SiLU()\n",
    "        self.normalization = LayerNormalization(dim = 1)\n",
    "        self.variance      = torch.sqrt(torch.tensor(2))\n",
    "\n",
    "        # Temporal Projection\n",
    "        self.time_projection = nn.Linear(in_features = self.frequencies * 2, out_features = input_channels, bias = False)\n",
    "\n",
    "        # Convolutions\n",
    "        self.conv1 = nn.Conv2d(in_channels  = input_channels,\n",
    "                               out_channels = input_channels,\n",
    "                               kernel_size  = 3,\n",
    "                               stride       = 1,\n",
    "                               padding      = 1)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels  = input_channels,\n",
    "                               out_channels = input_channels,\n",
    "                               kernel_size  = 3,\n",
    "                               stride       = 1,\n",
    "                               padding      = 1)\n",
    "\n",
    "    def forward(self, x, time):\n",
    "\n",
    "        # -------------------\n",
    "        #        Time\n",
    "        # -------------------\n",
    "        # 1. Initial information\n",
    "        b, c, x_res, y_res = x.shape\n",
    "\n",
    "        # 2. Temporal Projection\n",
    "        encoded_time = self.time_projection(time)\n",
    "        encoded_time = self.activation(encoded_time)\n",
    "\n",
    "        # 3. Reshaping the time encoding\n",
    "        encoded_time = encoded_time[:, :, None, None]\n",
    "\n",
    "        # -------------------\n",
    "        #        Spatial\n",
    "        # -------------------\n",
    "        # 1. Adding temporal information (broadcasting)\n",
    "        x_residual = x + encoded_time\n",
    "\n",
    "        # 2. Normalization\n",
    "        x_residual = self.normalization(x_residual)\n",
    "\n",
    "        # 3. Convolution (1)\n",
    "        x_residual = self.conv1(x_residual)\n",
    "\n",
    "        # 4. Activation\n",
    "        x_residual = self.activation(x_residual)\n",
    "\n",
    "        # 5. Convolution (2)\n",
    "        x_residual = self.conv2(x_residual)\n",
    "\n",
    "        # 6. Adding the residual\n",
    "        x = x + x_residual\n",
    "\n",
    "        # 7. Keeping unit variance\n",
    "        return x / self.variance\n",
    "\n",
    "    def count_parameters(self,):\n",
    "        r\"\"\"Determines the number of trainable parameters in the model\"\"\"\n",
    "        return int(sum(p.numel() for p in self.parameters() if p.requires_grad))\n",
    "\n",
    "class TimeResidual_UNET(nn.Module):\n",
    "    r\"\"\"A time residual UNET for time series forecasting\"\"\"\n",
    "\n",
    "    def __init__(self, input_channels: int, output_channels: int, frequencies: int, scaling: int = 1):\n",
    "        super(TimeResidual_UNET, self).__init__()\n",
    "\n",
    "        # Initializations\n",
    "        self.frequencies      = frequencies\n",
    "        self.input_channels   = input_channels\n",
    "        self.output_channels  = output_channels\n",
    "\n",
    "        # 1. Input (lifting)\n",
    "        self.input_conv = nn.Conv2d(in_channels = self.input_channels, out_channels = 32 * scaling, kernel_size = 3, stride = 1, padding = 1)\n",
    "\n",
    "        # 2. Downsampling\n",
    "        #\n",
    "        # Time Residual Blocks (1)\n",
    "        self.downsample_11_residuals = TimeResidual_Block(input_channels  = 32 * scaling,     frequencies = self.frequencies)\n",
    "        self.downsample_12_residuals = TimeResidual_Block(input_channels  = 32 * scaling,     frequencies = self.frequencies)\n",
    "        self.downsample_21_residuals = TimeResidual_Block(input_channels  = 32 * scaling * 2, frequencies = self.frequencies)\n",
    "        self.downsample_22_residuals = TimeResidual_Block(input_channels  = 32 * scaling * 2, frequencies = self.frequencies)\n",
    "        self.downsample_31_residuals = TimeResidual_Block(input_channels  = 32 * scaling * 4, frequencies = self.frequencies)\n",
    "        self.downsample_32_residuals = TimeResidual_Block(input_channels  = 32 * scaling * 4, frequencies = self.frequencies)\n",
    "        self.downsample_41_residuals = TimeResidual_Block(input_channels  = 32 * scaling * 8, frequencies = self.frequencies)\n",
    "        self.downsample_42_residuals = TimeResidual_Block(input_channels  = 32 * scaling * 8, frequencies = self.frequencies)\n",
    "\n",
    "        # Convolutions (downsampling)\n",
    "        self.downsample_1_conv = nn.Conv2d(in_channels = 32 * scaling,     out_channels = 32 * scaling * 2, kernel_size = 2, stride = 2)\n",
    "        self.downsample_2_conv = nn.Conv2d(in_channels = 32 * scaling * 2, out_channels = 32 * scaling * 4, kernel_size = 2, stride = 2)\n",
    "        self.downsample_3_conv = nn.Conv2d(in_channels = 32 * scaling * 4, out_channels = 32 * scaling * 8, kernel_size = 2, stride = 2)\n",
    "\n",
    "        # 3. Upsampling\n",
    "        #\n",
    "        # Used for upsampling instead of transposed convolutions\n",
    "        self.upsample = nn.Upsample(scale_factor = (2, 2))\n",
    "\n",
    "        # Convolutions (projection)\n",
    "        self.projection_1 = nn.Conv2d(in_channels = 32 * scaling * (8 + 4), out_channels = 32 * scaling * 4, kernel_size = 3, padding = 1)\n",
    "        self.projection_2 = nn.Conv2d(in_channels = 32 * scaling * (4 + 2), out_channels = 32 * scaling * 2, kernel_size = 3, padding = 1)\n",
    "        self.projection_3 = nn.Conv2d(in_channels = 32 * scaling * (2 + 1), out_channels = 32 * scaling    , kernel_size = 3, padding = 1)\n",
    "\n",
    "        # Time Residual Blocks (2)\n",
    "        self.upsample_11_residuals = TimeResidual_Block(input_channels = 32 * scaling * 4, frequencies = self.frequencies)\n",
    "        self.upsample_12_residuals = TimeResidual_Block(input_channels = 32 * scaling * 4, frequencies = self.frequencies)\n",
    "        self.upsample_21_residuals = TimeResidual_Block(input_channels = 32 * scaling * 2, frequencies = self.frequencies)\n",
    "        self.upsample_22_residuals = TimeResidual_Block(input_channels = 32 * scaling * 2, frequencies = self.frequencies)\n",
    "        self.upsample_31_residuals = TimeResidual_Block(input_channels = 32 * scaling    , frequencies = self.frequencies)\n",
    "        self.upsample_32_residuals = TimeResidual_Block(input_channels = 32 * scaling    , frequencies = self.frequencies)\n",
    "\n",
    "        # 4. Output (We use a linear to mix accross channels, a convolution mix spatially and introduce bias at the corners)\n",
    "        self.output_linear = nn.Linear(in_features = 32 * scaling, out_features = self.output_channels, bias = False)\n",
    "\n",
    "        # Normalization\n",
    "        self.normalization = LayerNormalization(dim = 1)\n",
    "\n",
    "    def forward(self, x, time):\n",
    "\n",
    "        # 1. Lifting\n",
    "        x = self.input_conv(x)\n",
    "\n",
    "        # 2. Downsampling\n",
    "        x = self.downsample_11_residuals(x, time)\n",
    "        x = self.downsample_12_residuals(x, time)\n",
    "\n",
    "        x1 = self.downsample_1_conv(x)\n",
    "        x1 = self.downsample_21_residuals(x1, time)\n",
    "        x1 = self.downsample_22_residuals(x1, time)\n",
    "\n",
    "        x2 = self.downsample_2_conv(x1)\n",
    "        x2 = self.downsample_31_residuals(x2, time)\n",
    "        x2 = self.downsample_32_residuals(x2, time)\n",
    "\n",
    "        x3 = self.downsample_3_conv(x2)\n",
    "        x3 = self.downsample_41_residuals(x3, time)\n",
    "        x3 = self.downsample_42_residuals(x3, time)\n",
    "        x3 = self.normalization(x3)\n",
    "\n",
    "        # 3. Upsampling\n",
    "        x3 = self.normalization(x3) # Good Practice for conditioning the data with diffusion, etc.\n",
    "        x3 = self.upsample(x3)\n",
    "\n",
    "        x2 = torch.cat([x3, x2], dim = 1)\n",
    "        x2 = self.projection_1(x2)\n",
    "        x2 = self.upsample_11_residuals(x2, time)\n",
    "        x2 = self.upsample_12_residuals(x2, time)\n",
    "        x2 = self.normalization(x2)\n",
    "        x2 = self.upsample(x2)\n",
    "\n",
    "        x1 = torch.cat([x2, x1], dim = 1)\n",
    "        x1 = self.projection_2(x1)\n",
    "        x1 = self.upsample_21_residuals(x1, time)\n",
    "        x1 = self.upsample_22_residuals(x1, time)\n",
    "        x1 = self.normalization(x1)\n",
    "        x1 = self.upsample(x1)\n",
    "\n",
    "        x = torch.cat([x1, x], dim = 1)\n",
    "        x = self.projection_3(x)\n",
    "        x = self.upsample_31_residuals(x, time)\n",
    "        x = self.upsample_32_residuals(x, time)\n",
    "\n",
    "        # 4. Output\n",
    "        x = self.output_linear(torch.permute(x, (0, 2, 3, 1)))\n",
    "\n",
    "        # 5. Adding separate channels for mean and log(var)\n",
    "        return torch.permute(x, (0, 3, 1, 2))\n",
    "\n",
    "    def count_parameters(self,):\n",
    "        r\"\"\"Determines the number of trainable parameters in the model\"\"\"\n",
    "        return int(sum(p.numel() for p in self.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Diffusion_UNET(nn.Module):\n",
    "    r\"\"\"A diffusion UNET for time series forecasting\"\"\"\n",
    "\n",
    "    def __init__(self, diffusion_steps: int, diffusion_scheduler: float = 0.01511, diffusion_variance: float = 0.01, scaling: int = 1, frequencies: int = 32, device: str = 'cpu'):\n",
    "        super(Diffusion_UNET, self).__init__()\n",
    "\n",
    "        # Initialization\n",
    "        self.diffusion_steps     = diffusion_steps\n",
    "        self.diffusion_scheduler = diffusion_scheduler\n",
    "        self.diffusion_variance  = diffusion_variance\n",
    "        self.frequencies         = frequencies\n",
    "        self.device              = device\n",
    "\n",
    "        # Number of inputs has changed, now its input and conditional\n",
    "        self.model               = TimeResidual_UNET(input_channels = 2, output_channels = 1, frequencies = self.frequencies, scaling = scaling)\n",
    "\n",
    "        # Possible diffusion steps\n",
    "        steps_t = torch.arange(1, self.diffusion_steps + 1, dtype = torch.float32)\n",
    "\n",
    "        # Computing encoding for time\n",
    "        self.encoded_steps = time_encoding(steps_t[:, None], self.frequencies)[:, 0]\n",
    "\n",
    "        # Precomputing constants\n",
    "        betas  = torch.ones((self.diffusion_steps, 1), dtype = torch.float32) * diffusion_scheduler\n",
    "        alphas = torch.pow(1 - diffusion_scheduler, steps_t)[:, None]\n",
    "        self.sqrt_alphas           = torch.sqrt(alphas)\n",
    "        self.sqrt_one_minus_alphas = torch.sqrt(1 - alphas)\n",
    "\n",
    "        # Computing constant for diffusion (in front of latent zt and noise predicted)\n",
    "        self.latent_constant_zt    = 1 / (torch.sqrt(1 - betas))\n",
    "        self.latent_constant_noise = betas / (torch.sqrt(1 - alphas) * torch.sqrt(1 - betas))\n",
    "\n",
    "    def generate_latent(self, x, noise, diffusion_steps):\n",
    "        \"\"\"Used to generate the latent variable z_t given the input x\"\"\"\n",
    "\n",
    "        # Extracting constants\n",
    "        sqrt_alphas           = self.sqrt_alphas[diffusion_steps[:, 0]][:, :, None, None].expand(-1, -1, *x.shape[2:])\n",
    "        sqrt_one_minus_alphas = self.sqrt_one_minus_alphas[diffusion_steps[:, 0]][:, :, None, None].expand(-1, -1, *x.shape[2:])\n",
    "\n",
    "        # Computing the latent variable z_t\n",
    "        return sqrt_alphas * x + sqrt_one_minus_alphas * noise\n",
    "\n",
    "    def predict(self, z, diffusion_steps):\n",
    "        return self.model(z, self.encoded_steps[diffusion_steps[:,0]].to(self.device))\n",
    "\n",
    "    def generate_samples(self, steps = None):\n",
    "        \"\"\"Used to generate samples from the model\"\"\"\n",
    "\n",
    "        # Libraries\n",
    "        from tqdm import tqdm\n",
    "        from time import sleep\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            # Sampling the initial noise\n",
    "            zt = torch.normal(0, 1, (10, 1, 32, 32), device = self.device)\n",
    "\n",
    "            # Pushing to the device\n",
    "            self.latent_constant_zt    = self.latent_constant_zt.to(self.device)\n",
    "            self.latent_constant_noise = self.latent_constant_noise.to(self.device)\n",
    "\n",
    "            # Computing the conditional\n",
    "            conditioning = torch.linspace(0, 9, 10)[:, None, None, None].to(self.device).expand(-1, 1, 32, 32)\n",
    "\n",
    "            # Reverse process\n",
    "            for t in tqdm(range(self.diffusion_steps - 1 if steps == None else steps, 0, -1)):\n",
    "\n",
    "                # Genereting encoded timesteps\n",
    "                diffusion_steps = torch.ones(10, 1, dtype=torch.int64) * t\n",
    "\n",
    "                # Removing the noise\n",
    "                zt_hat = self.latent_constant_zt[t] * zt - self.latent_constant_noise[t] * self.predict(torch.cat([conditioning, zt], dim = 1), diffusion_steps)\n",
    "\n",
    "                # Generating noise\n",
    "                noise = torch.normal(0, 1, zt_hat.shape).to(self.device)\n",
    "\n",
    "                # Adding a bit of noise for stochasticity but not on the last step\n",
    "                zt = zt_hat + self.diffusion_variance * noise if t > 1 else zt_hat\n",
    "\n",
    "            # Returning the generated samples\n",
    "            return zt\n",
    "\n",
    "    def count_parameters(self,):\n",
    "        r\"\"\"Determines the number of trainable parameters in the model\"\"\"\n",
    "        return self.model.count_parameters()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"color:#5A7D9F;\">\n",
    "<p align=\"center\">\n",
    "    <b style=\"font-size:2vw; color:#5A7D9F; font-weight:bold;\">\n",
    "    <center>Training</center>\n",
    "    </b>\n",
    "</p>\n",
    "<hr style=\"color:#5A7D9F;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------- PARAMETERS\n",
    "#\n",
    "# Total steps in diffusion\n",
    "T = 200\n",
    "\n",
    "# Diffusion scheduler\n",
    "beta = 0.0125\n",
    "\n",
    "# Diffusion variance\n",
    "diffusion_variance = 0.005\n",
    "\n",
    "# Number of frequencies for the encoding\n",
    "frequencies = 128\n",
    "\n",
    "# Scaling\n",
    "scaling = 1\n",
    "\n",
    "# Learning rate\n",
    "lr = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------- MODEL\n",
    "#\n",
    "# Checking if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device: \", device)\n",
    "\n",
    "# Loading the architecture\n",
    "model = Diffusion_UNET(diffusion_steps = T, diffusion_variance = diffusion_variance, scaling = scaling, frequencies = frequencies, device = device)\n",
    "model.to(device)\n",
    "\n",
    "# Total number of parameters\n",
    "nn_params = model.count_parameters()\n",
    "print(\"Total number of parameters: \", nn_params/1e6, \"M\")\n",
    "\n",
    "# Total number of available GPUs\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(\"Available GPUs: \", num_gpus)\n",
    "\n",
    "# Loading the optimizer\n",
    "optimizer  = optim.Adam(model.parameters(), lr =lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(20):\n",
    "\n",
    "    for x, c in tqdm(train_loader):\n",
    "\n",
    "        # ------ Preprocessing -----\n",
    "        #\n",
    "        # Sampling uniformly diffusion steps\n",
    "        diffusion_steps = torch.randint(0, T, (x.shape[0], 1))\n",
    "\n",
    "        # Sampling noise\n",
    "        noise = torch.normal(0, 1, x.shape)\n",
    "\n",
    "        # Generating latent representations of the data\n",
    "        z_t = model.generate_latent(x, noise, diffusion_steps)\n",
    "\n",
    "        # Pushing to device-\n",
    "        z_t, noise, diffusion_steps =  z_t.to(device), noise.to(device), diffusion_steps\n",
    "\n",
    "        # Creation of the conditioning\n",
    "        conditioning = c[:, None, None, None].expand(-1, 1, x.shape[2], x.shape[3]).to(device)\n",
    "\n",
    "        # Adding the conditioning\n",
    "        z_t = torch.cat([conditioning, z_t], dim = 1)\n",
    "\n",
    "        # ----- Training -----\n",
    "        #\n",
    "        # Prediction\n",
    "        noise_pred = model.predict(z_t, diffusion_steps)\n",
    "\n",
    "        # Loss\n",
    "        loss = torch.pow(noise_pred - noise, 2).mean()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Information over terminal\n",
    "    print(\"Loss: \", loss.item())\n",
    "    samples = model.generate_samples(steps = T - 1).cpu()\n",
    "    plt.figure(figsize = (10, 10))\n",
    "    for i in range(10):\n",
    "        plt.subplot(2, 5, i + 1)\n",
    "        plt.title(f\"C = {i}\")\n",
    "        plt.imshow(samples[i].squeeze().numpy(), cmap='viridis')\n",
    "        plt.tight_layout()\n",
    "        plt.axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for steps in [25, 50, 100, 125, 150, 175]:\n",
    "    samples = model.generate_samples(steps = steps).cpu()\n",
    "    plt.figure(figsize = (10, 10))\n",
    "    for i in range(10):\n",
    "        plt.subplot(2, 5, i + 1)\n",
    "        plt.title(f\"C = {i}\")\n",
    "        plt.imshow(samples[i].squeeze().numpy(), cmap='viridis')\n",
    "        plt.tight_layout()\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esa",
   "language": "python",
   "name": "esa"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
